\documentclass{article} % this tells LaTeX to make an article (as opposed to a book, for example)

\setlength{\parindent}{0pt}

\usepackage{amsmath, % this adds functions for formatting equations nicely
      amssymb,
      multirow,
      multicol,
      amsthm,
      todonotes, % add to do notes with \todo command
      mathtools, } % this gives us lots of greek symbols

\setlength\parindent{0pt} % sets indent to zero
\setlength{\parskip}{10pt} 
      
\newtheorem{definition}{Definition} % this lets us create definition blocks as below
\newtheorem{remark}{Remark}

\title{Revealed Preference Theory}

\begin{document}  

\maketitle

\section{Setup and Definitions}

\textbf{Setup:}
Let $X=(x_{1}, x_{2}, \ldots, x_{n})$ and $Y=(y_{1}, y_{2}, \ldots, y_{n})$ be vectors representing bundles of goods chosen by the same individual at two different times. Each $x_{i}\in X$ and $y_{i}\in Y$ denote the quantity of good $i$ chosen in bundle X and Y, respectively. Further, let $P=(p_{1}, p_{2}, \ldots, p_{n})$ be the vector of available prices at the time when bundle $X$ was chosen such that $p_{i}\in P$ denotes the price of good $i$. Then, consider the following definitions:

\begin{definition}
We say that $X$ is \textbf{directly revealed preferred (DRP)} to $Y$ if $P\cdot Y \leq P\cdot X$ (i.e. $p_{1}y_{1}+p_{2}y_{2}+\ldots+p_{n}y_{n}\leq p_{1}x_{1}+p_{2}x_{2}+\ldots+p_{n}x_{n}$). Intuitively, $X$ is directly revealed preferred to $Y$ if $Y$ was affordable at the prices when $X$ was chosen.
\end{definition}

\begin{definition}
We say that $X$ is \textbf{strictly directly revealed preferred (SDRP)} to $Y$ if $P\cdot Y < P\cdot X$ (i.e. $p_{1}y_{1}+p_{2}y_{2}+\ldots+p_{n}y_{n}<p_{1}x_{1}+p_{2}x_{2}+\ldots+p_{n}x_{n}$). Intuitively, $X$ is strictly directly revealed preferred to $Y$ if $Y$ was cheaper than $X$ at the prices when $X$ was chosen.
\end{definition}

\begin{definition}
Consider three bundles, $X$, $Y$, and $Z$. Then $X$ is \textbf{indirectly revealed preferred (IRP)} to $Z$ if $X$ is directly revealed preferred to $Y$ and $Y$ is directly revealed preferred to $Z$.
\todo{Update to allow for chains of arbitrary length.}
\end{definition}

\begin{definition}
We say $X$ is \textbf{revealed preferred (RP)} to $Y$ if $X$ is either directly revealed preferred or indirectly revealed preferred to $Y$.
\end{definition}

\begin{definition}
A set of choice data satisfies the \textbf{Weak Axiom of Revealed Preferences (WARP)} if for all bundles $X$ and $Y$, if $X$ is directly revealed preferred to $Y$, then $Y$ is not directly revealed preferred to $X$.
\end{definition}

\begin{remark}
If a consumer exhibits maximizing behavior, then their set of choice data will satisfy WARP. However, the converse isn't necessarily true when X and Y represent bundles from commodity spaces higher than two dimensions (i.e. there are more than two available goods and thus, $X$, $Y$, and $P$ are n-dimensional vectors such that $n> 2$\footnote{Rose, H. (1958). Consistency of preference: The two-commodity case. Review of Economic Studies, 25(2), 124â€“125. }). 
\todo{Move citation to bibligraphy instead of footnote.}
\end{remark}

\begin{definition}
A set of choice data satisfies the \textbf{Strong Axiom of Revealed Preferences (SARP)} if for all bundles $X$ and $Y$, if $X$ is revealed preferred (directly or indirectly) to $Y$, then $Y$ is not revealed preferred (directly or indirectly) to $X$.
\end{definition}

\begin{remark}
A consumer's set of choice data will satisfy SARP if and only if they exhibit maximizing behavior. 
\todo{Add citation.}
\end{remark}

\begin{definition}
A set of choice data satisfies the \textbf{Weak Generalized Axiom of Revealed Preferences (WGARP)} if for all bundles $X$ and $Y$, if $X$ is directly revealed preferred to $Y$, then $Y$ is not strictly directly revealed preferred to $X$.
\end{definition}

\begin{definition}
A set of choice data satisfies the \textbf{Generalized Axiom of Revealed Preferences (GARP)} if for all bundles $X$ and $Y$, if $X$ is revealed preferred (directly or indirectly) to $Y$, then $Y$ is not strictly directly revealed preferred to $X$.
\end{definition}

\begin{remark}
The key difference between SARP and GARP requirements are that GARP allows consumers to have multiple bundles that maximizes utility under the same price levels (i.e. consumers are allowed to exhibit preferences that treat goods as perfect substitutes of each other).
\todo{Add citation.}
\end{remark}

The following table summarizes each of the axioms and their requirements. For all bundles $X$ and $Y$:

\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{ c|c|c } 
Axiom & If $\ldots$ & Then $\ldots$ \\\hline
WARP&$X$ is \textbf{DRP} to $Y$&$Y$ cannot be \textbf{DRP} to $X$ \\
SARP&$X$ is \textbf{RP} to $Y$&$Y$ cannot be \textbf{RP} to $X$ \\
WGARP&$X$ is \textbf{DRP} to $Y$&$Y$ cannot be \textbf{SDRP} to $X$ \\
GARP&$X$ is \textbf{RP} to $Y$&$Y$ cannot be \textbf{SDRP} to $X$
\end{tabular}
\end{center}
\renewcommand{\arraystretch}{1}

\begin{definition}
The \textbf{Critical Cost Efficiency Index (CCEI)} is the amount by which each budget
constraint must be relaxed in order to remove all violations of GARP. Lower values of CCEI indicates less rational choice data. Note that a CCEI score of 1 indicates a set of choice data satisfies GARP while CCEI approaches 0 as the revealed preferences become less rational. The actual calculations / formula for CCEI will be detailed in the following section.
\end{definition}

\section{Calculating the CCEI (upperbound)}
Suppose for an individual, we have $X_{1}, X_{2}, \ldots, X_{k}$ vectors representing $k$ bundles of $n$ goods chosen at $k$ different times so that $X_{i}\in\mathbb{R}^{n}_{+}$ for all $i\in\{1,2,\ldots,k\}$. Note that the technical notation can get a bit muddy, as $X_{i}=(x_{i_{1}}, x_{i_{2}}, \ldots, x_{i_{n}})$ where $x_{i_{j}}$ represents the quantity of good $j$ bought at time $i$. We also have $P_{1}, P_{2}, \dots, P_{k}$ vectors representing $k$ sets of available prices of $n$ goods at $k$ different times of that $P_{i}\in\mathbb{R}^{n}_{++}$ for all $i\in\{1,2,\ldots,k\}$. Similarly, $P_{i}=(p_{i_{1}}, p_{i_{2}}, \ldots, p_{i_{n}})$ where $p_{i_{j}}$ represents the price of good $j$ at time $i$.
\todo{Change notation here to have side-by-side subscripts instead of sub-subscripts, ie $p_{ij}$ instead of $p_{i_{j}}$.}


We then say $\forall i,j\in\{1,2,\ldots,k\}$ such that $i\not=j$, let:
    $$D_{ij}=\frac{P_{i}X_{j}}{P_{i}X_{i}}-1$$
Note that the above is only negative if and only if $X_{i}$ is strictly directly revealed preferred to $X_{j}$. That's equivalent of saying that although the full bundle of $X_{j}$ was available at the price vector of time $i$, the individual observed chose $X_{i}$ over $X_{j}$.


Then let
$$d_{ij}=max\{D_{ij}, D_{ji}\}$$
We note that $\forall i,j\in\{1,2,\ldots,k\}$ such that $i\not=j$, $d_{ij}<0$ if and only if we have a strictly directly revealed preference conflict. To see this, consider the following:

\begin{itemize}
    \item $D_{ij}<0$ if and only if $X_{i}$ is strictly directly revealed preferred to $X_{j}$ (as per the previous paragraph). 
    \item Similarly, $D_{ji}<0$ if and only if $X_{j}$ is strictly directly revealed preferred to $X_{i}$.
    \item $d_{ij}=max\{D_{ij}, D_{ji}\}<0$ if and only if $D_{ij}<0$ and $D_{ji}<0$, which means that $X_{i}$ is strictly directly revealed preferred to $X_{j}$ \textbf{and} $X_{j}$ is strictly directly revealed preferred to $X_{i}$. 
    \item Thus we have a strictly directly revealed preference conflict if $d_{ij} < 0$.
\end{itemize}

Essentially, $d_{ij}$ perfectly captures whether there is a strictly directly revealed preference conflict between bundles $X_{i}$ and $X_{j}$. Moreover, $d_{ij}$ also captures the degree of the less ``egregious" violation. That is, if $X_{i}$ is way more strictly directly revealed preferred to $X_{j}$ than $X_{j}$ is strictly directly revealed preferred to $X_{i}$, (i.e. if $D_{ij}<<0$ and $D_{ji}<0$), the second preference that caused the violation is less severe and can be more easily "fixed". Since $d_{ij}=max\{D_{ij}, D_{ji}\}<0$, the lesser violation $D_{ji}$ is captured instead.

We then let:
$$
e_{ij}=1-\max\{0,-d_{ij}\}
$$\todo{Redefine this with piecewise function instead of $\max$}

$e_{ij}$ is called the cross cost efficiency index between bundles $X_{i}$ and $X_{j}$. Note that if $d_{ij}\geq0$, then $-d_{ij}\leq0$ and we would get $e_{ij}=(1-0)=1$. Thus, if there are no strictly directly revealed preference conflicts between $X_{i}$ and $X_{j}$, we would get $e_{ij}=1$. If we have such a violation, then $-d_{ij}>0$ and hence we would get $e_{ij}=1+d_{ij}$ (note that $0<e_{ij}<1$ since $d_{ij}<0$). We claim that, by letting $\hat{ P_iX_i}=e_{ij}\cdot P_iX_i$ and $\hat{P_jX_j}=e_{ij}\cdot P_jX_j$ and using those values to recalculate a $\hat{d_{ij}}$, we would have $\hat{d_{ij}}\geq0$ and hence there are no strictly directly revealed preference violations. This can be seen in the example section and will be proved more rigorously towards the end of this manual. For now, it suffices to note that the lower the value of $e_{ij}$, the harder it is to ``fix" the violation between $X_{i}$ and $X_{j}$.


We then let:

$$e^{*}=\min_{\forall i,j, i\not=j}\{e_{ij}\}$$

$e^{*}$, as given above, would resolve all strictly directly revealed preference violations. The lowest value of $e_{ij}$ for all $i,j\in\{1,2,\ldots,k\}$ would solve the most ``egregious" cross bundle violation and would obviously solve all other remaining cross bundle violations.


\textbf{However, this $\mathbf{e^*}$ is merely an upper bound for the CCEI} as it only resolves cross bundle violations in the dataset. Essentially, $e^*$ would only resolve violations that involve direct revealed preferences. Another value, say $e^*_{2}$, might be needed to resolve indirect revealed preferences and if $e^*_{2}\leq e^*$, then $e^*_{2}$ would be our actual CCEI as it resolve both indirect and direct revealed preference violations (and thus passing GARP).

\section{Examples}

Consider the following data for a single consumer in 3 different time periods:

\begin{center}
\begin{tabular}{ cccccc } 
Choice Number & $p_{1}$ & $p_{2}$ & $x_{1}$ & $x_{2}$ \\
1&1&2&1&2 \\
2&2&1&2&1 \\
3&1&1&2&2
\end{tabular}
\end{center}

Then, $\forall n\in\{1,2,3\}$, we calculate $P_{n}X_{n}$ and $\forall i,j\in\{1,2,3\}$ such that $i\not=j$, we calculate $P_{j}X_{i}$. We then get the following expenditure matrix:

\begin{center}
\begin{tabular}{ cccccc } 
Choice Number & Budget 1 & Budget 2 & Budget 3 \\
1&5&4&3 \\
2&4&5&3 \\
3&6&6&4
\end{tabular}
\end{center}

Looking at the last 3 columns of the matrix, we see that $\forall n\in\{1,2,3\}$, the $(n,n)$ element on the matrix represents the cost of the bundle actually chosen on time period $n$ (i.e. $P_{n}X_{n}$). Additionally, $\forall i,j\in\{1,2,3\}$ such that $i\not=j$, the $(i,j)$ element on the matrix, we calculate the cost of bundle $i$ at available price vector $j$ (i.e. $P_{j}X_{i}$).
\bigskip

From the above expenditure matrix, we can directly check if there are an violations for each of our axioms. Using our definition for directly revealed preferences, we put a $(*)$ next to each cell $(j,i)$ such that $\forall i,j\in\{1,2,3\}$ where $i\not=j$, $P_iX_j\leq P_iX_i$ (i.e. element $(j,i)\leq$ element $(i,i)$). This would indicate bundle $i$ DRP bundle $j$. We then get the following:

\begin{center}
\begin{tabular}{ cccccc } 
Choice Number & Budget 1 & Budget 2 & Budget 3 \\
1&5&4*&3* \\
2&4*&5&3* \\
3&6&6&4
\end{tabular}
\end{center}

Note then that the above fails all our axioms. Since there's a $(*)$ on element $(2,1)$ and element $(1,2)$, indicating that bundle 1 DRP bundle 2 and also bundle 2 DRP bundle 1, failing WARP and SARP. Additionally, if we less our $(*)$ requirement to be: put a $(*)$ next to each cell $(j,i)$ such that $\forall i,j\in\{1,2,3\}$ where $i\not=j$, $P_iX_j<P_iX_i$, we would still get the same matrix and this fail both WGARP and GARP.

\section{Appendix}

Here we prove our claim in \textbf{Section 2}.


We first let:


$$D_{ij}=\frac{P_iX_j}{P_iX_i}-1 \quad \text{and} \quad D_{ji}=\frac{P_jX_i}{P_jX_j}-1.$$



Suppose that $D_{ij}\leq D_{ji}<0$, which means that we have a violation but the strictly directly revealed preference of $X_j$ over $X_i$ is less severe than that of the inverse. Then we take:
$$d_{ij}=max\{D_{ij},D_{ji}\}$$

Note that, by construction, we will simply have:
$$d_{ij}=D_{ji}<0$$

We then can calculate:
$$e_{ij}=1+d_{ij}=1+D_{ji}$$

We then want to prove the following claim:


\textbf{Claim:} $\hat{D_{ji}}=\frac{P_jX_i}{(e_{ij})P_jX_j}-1\geq0$ and hence $\hat{d_{ij}}=max\{\hat{D_{ij}},\hat{D_{ji}}\}\geq0$


\begin{proof}
Recall that:

$$e_{ij}=1+D_{ji}=1+ \left (\frac{P_jX_i}{P_jX_j}-1 \right )=\frac{P_jX_i}{P_jX_j}$$

We then have:

$$\hat{D_{ji}}=\frac{P_jX_i}{(e_{ij})P_jX_j}-1=\frac{P_jX_i}{(\frac{P_jX_i}{P_jX_j})P_jX_j}-1$$

Simplifying:

$$\hat{D_{ji}}=\frac{P_jX_i}{(e_{ij})P_jX_j}-1=\frac{P_jX_i}{P_jX_i}-1=0\geq0$$

Hence, $\hat{D_{ji}}\geq0$ and we have $\hat{d_{ij}}=max\{\hat{D_{ij}},\hat{D_{ji}}\}\geq0$.

\end{proof}

\newpage

\section{Axioms of Revealed Preferences: A Graph Theoretical Approach}

\textbf{THIS FOLLOWING SECTION IS A DRAFT} mainly used to see if there's a faster way to test *ARP and calculate exact values of CCEI without binary search.


Let's begin by looking at the problem with a new perspective. Let $\mathcal{X}=\{X_1,\ldots,X_k\}$ and $\mathcal{P}=\{P_1,\ldots,P_k\}$ be sets of quantity and price vectors, respectively. We then have data set $(\mathcal{X},\mathcal{P})$. Then consider the following directed graphs:


\begin{minipage}{.5\linewidth}
$$G_d=\{V,E_d\}$$
\end{minipage}%
\begin{minipage}{.5\linewidth}
$$G_s=\{V,E_s\}$$
\end{minipage}


where:

$$V=\{(X_i,P_i)\ |\ \forall i\in\{1,\ldots,k\}\}\subseteq \mathcal{X}\times\mathcal{P}$$


such that $V_{i}=(X_i,P_i)$ fully captures all information about the bundle (both quantity and price) at time $i$. $V$ is then the vertex set for both the graphs. Then consider the following edge sets:

$$E_d=\{(V_i,V_j)\ |\ X_i R_D X_j,\ i.e.\ P_iX_i\geq P_iX_j\}$$

$$E_s=\{(V_i,V_j)\ |\ X_i R_{SD} X_j,\ i.e.\ P_iX_i> P_iX_j\}$$

(Note it might be better to just make the vertex set the set of $\mathcal{X}$ instead, but to be safe I just did an ordered pair between quantity and price).\todo{Lets make the vertexes just the bundles, not the prices.}


Then, it's apparent that $G_d$ is the directed graph for all directly revealed preference relationships and $G_s$ is the direct graph for all \textit{strictly} directly revealed preference relationships. If there's a path / edge from $V_i$ to $V_j$, we have $X_i$ is (strictly) directly revealed preferred to $X_j$ depending on the graph. Further, we shall note that $E_s\subseteq E_d$, as obviously all strictly directly revealed preferred relationships are directly revealed preferred.


We can then characterize our axioms with the following simple definitions:


\textbf{WARP:} If $(\mathcal{X},\mathcal{P})$ passes WARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(V_i,V_j)\in E_d$, then $(V_j, V_i)\not\in E_d$.


\textbf{SARP:} If $(\mathcal{X},\mathcal{P})$ passes SARP, then: $\forall V_i \in V$, there exists no directed walk from $V_i$ back to itself (I believe this is called a directed cycle, not quite sure on the exact terminology in graph theory).


\textbf{WGARP:} If $(\mathcal{X},\mathcal{P})$ passes WGARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(V_i,V_j)\in E_d$, then $(V_j, V_i)\not\in E_s$.


\textbf{GARP:} If $(\mathcal{X},\mathcal{P})$ passes GARP, then: $\forall i,j\in\{1,\dots,k\}$, if there exists a directed walk from $V_i$ to $V_j$, then $(V_j,V_i)\not\in E_s$.


\textbf{A New Way to Test WARP/SARP:} 

Let $A_d$ be the adjacency matrix of $G_d$. To be rigorous, we can generate $A_d$ from our data set in the following manner. First, create the following $(k\times k)$ matrix from $\mathcal{X}$ and $\mathcal{P}$:

\[
Exp = 
 \begin{pmatrix}
  P_1X_1 & P_2X_1 & \cdots & P_kX_1 \\
  P_1X_2 & P_2X_2 & \cdots & P_kX_2 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_k & P_2X_k & \cdots & P_kX_k
 \end{pmatrix}
\]

This above can be referred to as the expenditure matrix. Note that the $(i,j)^{th}$ entry of this matrix represents the cost of bundle $i$ at the price of time $j$ (i.e. $P_jX_i$). Then consider the following:

\[
Exp_{diag} = 
 \begin{pmatrix}
  P_1X_1 & P_2X_2 & \cdots & P_kX_k \\
  P_1X_1 & P_2X_2 & \cdots & P_kX_k \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_1 & P_2X_2 & \cdots & P_kX_k
 \end{pmatrix}
\]

In $Exp_{diag}$, every element in column $j$ is same for all $j$ and represents the actual cost of bundle $j$ at the price vector it was bought at (price vector of time $j$). Then consider the following:

\[
\alpha = Exp-Exp_{diag}
 \begin{pmatrix}
  0 & P_2X_1-P_2X_2 & \cdots & P_kX_1-P_kX_k \\
  P_1X_2-P_1X_1 & 0 & \cdots & P_kX_2-P_kX_k \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_k-P_1X_1 & P_2X_k-P_2X_2 & \cdots & 0
 \end{pmatrix}
\]

Then define a function $f:\mathbb{R}\to\{0,1\}$:

\[ 
f(x)=
    \begin{cases} 
      1 & x\leq0 \\
      0 & x>0
   \end{cases}
\]

We can then get the adjacency matrix by applying $f$ to every element of $\alpha$ except its diagonals and taking its transpose:

\[
A_d =
 \begin{pmatrix}
  0 & f(\alpha_{1,2}) & \cdots & f(\alpha_{1,k}) \\
  f(\alpha_{2,1}) & 0 & \cdots & f(\alpha_{2,k}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  f(\alpha_{k,1}) & f(\alpha_{k,2}) & \cdots & 0
 \end{pmatrix} ^T
\]

We shall briefly explain why this works intuitively:

\begin{itemize}
    \item $f(\alpha_{i,j})=1$ if and only if $\alpha_{i,j}\leq0$
    \item If $\alpha_{i,j}\leq0$, we know that $P_jX_i-P_jX_j\leq0$ and hence $P_jX_i\leq P_jX_j$
    \item Hence, by definition, we have $X_j R_D X_i$
    \item We conclude that $f(\alpha_{i,j})=1$ exactly when $X_j R_D X_i$
    \item However, a value of $1$ in $(i,j)$ position would indicate that a directed edge from node $i$ to node $j$, but since we want our edges to indicate directly revealed preferences, we want a value of $1$ at the $(j,i)$ positions. Hence, we take the transpose and get $A_{d_{j,i}}=f(\alpha_{i,j})=1$ if and only if $X_j R_D X_i$, which is our desired adjacency matrix.
\end{itemize}

Now we're ready to use matrix operations on $A_d$ to easily test for WARP and SARP.


We know that the number of directed walks from any node $i$ to node $j$ of length $k$ is given by $A^k_d$ for adjacency matrix $A_d$ (this can be proven by induction). 


\textbf{WARP:} We defined WARP requirements as followed: If $(\mathcal{X},\mathcal{P})$ passes WARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(V_i,V_j)\in E_d$, then $(V_j, V_i)\not\in E_d$. Note that this is true exactly when there \textit{isn't} a directed walk of length 2 from any node $i$ back to itself (node $i$). Hence we say $(\mathcal{X},\mathcal{P})$ passes WARP if and only if the main diagonal of matrix $A_d^2$ is a vector of 0, i.e.

\[
diag(A_d^2) =
 \begin{pmatrix}
  0 \\
  0 \\
  \vdots \\
  0
 \end{pmatrix}
\]

\textbf{SARP:} We defined SARP requirements as followed: If $(\mathcal{X},\mathcal{P})$ passes SARP, then: $\forall V_i \in V$, there exists no directed walk from $V_i$ back to itself. So we have to account for walks of \textit{any length}. Consider the following:

$$B_d=\sum_{i=2}^{k}A_d^i$$

$B_d$'s $(i,j)^{th}$ element the number of directed walks of at least length 2 (since the maximum length of a walk is $k$ without looping around again, the cardinality of the vertex set) from node $i$ to node $j$. The values of the main diagonal is then the number of directed walks of any length (at least 2) from node $i$ back to itself. Then $(\mathcal{X},\mathcal{P})$ passes SARP if and only if the main diagonal of matrix $B_d=\sum_{i=2}^{k}A_d^i$ is a vector of 0, i.e.:

\[
diag(B_d) = diag(\sum_{i=2}^{k}A_d^i)=
 \begin{pmatrix}
  0 \\
  0 \\
  \vdots \\
  0
 \end{pmatrix}
\]

To test for WGARP and GARP, we would need a second adjacency matrix to reflect $G_s$. Let's call it $A_s$. Since the edge set $E_s$ is pretty similar (in fact a subset of) to $E_d$, we just have to make a slight adjustment to $A_d$'s construction. First we take $\alpha$ as before, since our expenditure matrix (which represents the same vertex set) doesn't change. Then we define a function $g:\mathbb{R}\to\{0,1\}$:

\[ 
g(x)=
    \begin{cases} 
      1 & x<0 \\
      0 & x\geq0
   \end{cases}
\]

Then we can define $A_s$ as followed:

\[
A_s =
 \begin{pmatrix}
  0 & g(\alpha_{1,2}) & \cdots & g(\alpha_{1,k}) \\
  g(\alpha_{2,1}) & 0 & \cdots & g(\alpha_{2,k}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  g(\alpha_{k,1}) & g(\alpha_{k,2}) & \cdots & 0
 \end{pmatrix} ^T
\]

The reasoning behind how $A_s$ works is the same as the rationale behind $A_d$ except the conditions of $g$ are set such that the relationships between bundle $i$ and bundle $j$ need to be SDRP in order for $(i,j)$ to be 1. Then the following are ways in which we can test for WGARP and GARP.


\textbf{WGARP:} We can test for WGARP by multiplying $A_d$ and $A_s$ by matrix multiplication.


\textit{Claim:} A data set $(\mathcal{X}, \mathcal{P})$ passes WGARP if and only if the main diagonal of $A_d\times A_s$ is a vector of 0.

\begin{proof}
Recall that: If $(\mathcal{X},\mathcal{P})$ passes WGARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(V_i,V_j)\in E_d$, then $(V_j, V_i)\not\in E_s$. Which, when put in context of adjacency matrices, means that $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(i,j)^{th}$ position of $A_d$ ($A_{d_{i,j}}$) is equal to one, then the $(j,i)^{th}$ position of $A_s$ ($A_{s_{i,j}}$) cannot be equal to 1. We then consider $A_d\times A_s$ and its main diagonal. 


$\forall i\in\{1,\ldots,k\}$, let $\delta_i$ be the vector of the $i^{th}$ row of $A_d$ and $\xi_i$ be the vector of the $i^{th}$ column of $A_s$. Then we have $(A_d\times A_s)_{i,i}=\delta_i\cdot\xi_i$. Note that $\delta_{i_i}=0$ and $\xi_{i_i}=0$ since originally $A_d$ and $A_s$ have all 0's across their main diagonals.


We then know that, $\forall i\in\{1,\ldots,k\}$, $(A_d\times A_s)_{i,i}=0$ (i.e. the main diagonal of $A_d\times A_s$ is a zero vector) if and only if $\delta_i\cdot\xi_i=0$. Further, $\delta_i\cdot\xi_i=0$ if and only if $\forall j \in\{1,\ldots,k\}$ such that $i\not=j$, $\delta_{i_j}\cdot \xi_{i_j}=0$ (since we know that $\delta_{i_i}=\xi_{i_i}=0$, they won't contributed to the overall sum, which is what we want anyways by the WGARP rules). We then know that $\delta_{i_j}\cdot \xi_{i_j}=0$ if and only if $\delta_{i_j}=0$ or $\xi_{i_j}=0$. Further, "$\delta_{i_j}=0$ or $\xi_{i_j}=0$" if and only if $\neg (\delta_{i_j}=\xi_{i_j}=1)$ (since all terms in $A_d$ and $A_s$ are either one or zero). Note that $\delta_{i_j}=A_{d_{i,j}}$ and $\xi_{i_j}=A_{s_{j,i}}$ (since $\xi_i$ is the vector of the $i^{th}$ \textit{column} of $A_s$). Hence, $\neg (\delta_{i_j}=\xi_{i_j}=1)$ if and only if $\neg (A_{d_{i,j}}=A_{s_{j,i}}=1)$, which happens if $A_{d_{i,j}}=1$, then $A_{s_{j,i}}=0\not=1$. Note that this is the exact definition of passing WGARP. Summarizing this, we have:
\begin{flalign*}
\forall i\in\{1,\ldots,k\}, (A_d\times A_s)_{i,i}=0 &\iff \delta_i\cdot\xi_i=0 &\\
&\iff \forall j \in\{1,\ldots,k\}\textrm{, }i\not=j,\ \delta_{i_j}\cdot \xi_{i_j}=0 &\\
&\iff \delta_{i_j}=0\textrm{ or }\xi_{i_j}=0 &\\
&\iff \neg (\delta_{i_j}=\xi_{i_j}=1) &\\
&\iff \neg (A_{d_{i,j}}=A_{s_{j,i}}=1) &\\
&\iff (A_{d_{i,j}}=1 \implies A_{s_{j,i}}=0\not=1)
\end{flalign*}

Hence, $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(A_d\times A_s)_{i,i}=0 \iff (A_{d_{i,j}}=1 \implies A_{s_{j,i}}=0\not=1)$. Note the first statement simply means the main diagonal of $A_d\times A_s$ is the zero vector and the second statement is the condition for passing WGARP. We then have "passing WGARP" $\iff$ "main diagonal of $A_d\times A_s$ is a zero vector". Thus our claim is proven (since $A\iff B$ is the same as $B\iff A$).
\end{proof}

\textbf{GARP:} We can test for GARP by finding multiplying $\sum_{i=1}^{k-1}A_{d}^{i}$ with $A_s$ by matrix multiplication.


Before we begin, consider the following lemma:


\textit{Lemma A:} Let $X_1, X_2, Y$ be $(m\times m)$ matrices such that:

\begin{minipage}{.5\linewidth}
\[
X_1 =
 \begin{pmatrix}
  x_{1,1} & x_{1,2} & \cdots & x_{1,m} \\
  x_{2,1} & x_{2,2} & \cdots & x_{2,m} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{m,1} & x_{m,2} & \cdots & x_{m,m}
 \end{pmatrix}
\]
\end{minipage}
\begin{minipage}{.5\linewidth}
\[
X_2 =
 \begin{pmatrix}
  0 & x_{1,2} & \cdots & x_{1,m} \\
  x_{2,1} & 0 & \cdots & x_{2,m} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{m,1} & x_{m,2} & \cdots & 0
 \end{pmatrix}
\]
\end{minipage}
and 

\[
Y =
 \begin{pmatrix}
  0 & y_{1,2} & \cdots & y_{1,m} \\
  y_{2,1} & 0 & \cdots & y_{2,m} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  y_{m,1} & y_{m,2} & \cdots & 0
 \end{pmatrix}
\]

Then the main diagonals of $X_1\times Y$ and $X_2\times Y$ are the same (denoted $diag(X_1\times Y)=diag(X_2\times Y)$).

\begin{proof}
Fix an arbitrary $i\in\{1,\ldots,m\}$. Then let $Z_1=X_1\times Y$ and $Z_2=X_2\times Y$. We want to show that $Z_{1_{i,i}}=Z_{2_{i,i}}$. Note that $Z_{1_{i,i}}=x_{i,1}y_{1,i}+x_{i,2}y_{2,i}+\ldots+x_{i,i}y_{i,i}+\ldots+x_{i,m}y_{m,i}$ and that $Z_{2_{i,i}}=x_{i,1}y_{1,i}+x_{i,2}y_{2,i}+\ldots+0y_{i,i}+\ldots+x_{i,m}y_{m,i}$. The only difference between those are that for the first equation, we have $x_{i,i}y_{i,i}$ as the $i^{th}$ term and the second equation, we have $0y_{i,i}$ as the $i^{th}$ term. However, since $y_{i,i}=0$, we have that $x_{i,i}y_{i,i}=0y_{i,i}=0$ and thus $Z_{1_{i,i}}=Z_{2_{i,i}}$.
\end{proof}

\textit{Claim:} A data set $(\mathcal{X},\mathcal{P})$ passes GARP if and only if the main diagonal of $\sum_{i=1}^{k-1}A_{d}^{i}\times A_s$ is a vector of 0.

\begin{proof}
The proof of this will look very similar to the proof of WGARP, except that the elements in $\sum_{i=1}^{k-1}A_{d}^{i}$ will no longer be binary $(\{0,1\})$. However, they are all $\geq0$. Let $\beta_d=\sum_{i=1}^{k-1}A_{d}^{i}$, then $\beta_{d_{i,j}}$ would be the number of directed walks between node $i$ and node $j$ of any length up to $k-1$. Note that we chose to sum up to $k-1$ since anything past that, we would definitely have a cycle and summing past that point is redundant.


We want to make sure that if there's a directed walk from node $i$ to node $j$ in $G_d$, there isn't a directed edge between node $j$ and node $i$ in $G_s$. Formally, this is just our definition for GARP: If $(\mathcal{X},\mathcal{P})$ passes GARP, then: $\forall i,j\in\{1,\dots,k\}$, if there exists a directed walk from $V_i$ to $V_j$, then $(V_j,V_i)\not\in E_s$. This is similar to WGARP in the sense that we're simply replacing the directed edge in the "if" statement to a directed walk. Note then that we have $\beta_d=\sum_{i=1}^{k-1}A_{d}^{i}$, which captures the existence of any directed walks between any two nodes. Hence if $\beta_{d_{i,j}}>0$, there exists at least 1 directed walk between node $i$ and node $j$ on graph $G_d$. We can then use our proof for WGARP and \textit{Lemma A} to construct the rest of this proof.


In our proof of WGARP, we see that $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(A_d\times A_s)_{i,i}=0 \iff (A_{d_{i,j}}=1 \implies A_{s_{j,i}}=0\not=1)$. There are two changes we need to make to this statement to fit our GARP claim. 

First, we see that we're no longer working with $A_d$ but working with $\beta_d$, which represents directed walks instead of edges. $\beta_d$ is also no longer filled with binary elements $\in\{0,1\}$. We can tweak our statement to fit these conditions by stating: $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(\beta_d\times A_s)_{i,i}=0 \iff (\beta_{d_{i,j}}>0 \implies A_{s_{j,i}}=0)$. Now, $\beta_{d_{i,j}}>0$ simply means there exists at least one path from $i,j$, which is what we want.

Second, when we constructed our proof of WGARP, we noted that "since we know that $\delta_{i_i}=\xi_{i_i}=0$ (i.e. $A_{d_{i,i}}=0$ and $A_{s_{i,i}}=0$), they won't contributed to the overall sum, which is what we want anyways by the WGARP rules". However, this no longer holds as we could have $\beta_{d_{i,i}}\not=0$ for some $i$ if there's any directed cycles in $G_d$ (but we don't care about those in GARP, per the definition of the axiom). But we do still have $A_{s_{i,i}}=0$ for all $i$ and thus, by \textit{Lemma A}, we don't need to "clear-out" the diagonals of $\beta_d$ before multiplying. Hence, our statement above still holds. 

Thus, we have: $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(\beta_d\times A_s)_{i,i}=0 \iff (\beta_{d_{i,j}}>0 \implies A_{s_{j,i}}=0)$. The first statement simply means the main diagonal of $(\beta_d=\sum_{i=1}^{k-1}A_{d}^{i}\times A_s)$ is a zero vector and the second statement is the condition for passing GARP, hence our proof is complete (since $A\iff B$ is the same as $B\iff A$).
\end{proof}
\end{document}