\documentclass{article} % this tells LaTeX to make an article (as opposed to a book, for example)

\setlength{\parindent}{0pt}

\usepackage{amsmath, % this adds functions for formatting equations nicely
      amssymb,
      multirow,
      multicol,
      amsthm,
      todonotes, % add to do notes with \todo command
      mathtools,
      stmaryrd,
      tikz } % this gives us lots of greek symbols

\usepackage[round]{natbib} % extra functionality for citations


\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\setlength\parindent{0pt} % sets indent to zero
\setlength{\parskip}{10pt} 
      
\newtheorem{definition}{Definition} % this lets us create definition blocks as below
\newtheorem{remark}{Remark}

\title{Revealed Preference Theory}

\begin{document}

\maketitle

\section{Setup and Definitions}

\textbf{Setup:}
Let $X=(x_{1}, x_{2}, \ldots, x_{n})$ and $Y=(y_{1}, y_{2}, \ldots, y_{n})$ be vectors representing bundles of goods chosen by the same individual at two different times. Each $x_{i}\in X$ and $y_{i}\in Y$ denote the quantity of good $i$ chosen in bundle X and Y, respectively. Further, let $P=(p_{1}, p_{2}, \ldots, p_{n})$ be the vector of available prices at the time when bundle $X$ was chosen such that $p_{i}\in P$ denotes the price of good $i$. Then, consider the following definitions:

\begin{definition}
We say that $X$ is \textbf{directly revealed preferred (DRP)} to $Y$ if $P\cdot Y \leq P\cdot X$ (i.e. $p_{1}y_{1}+p_{2}y_{2}+\ldots+p_{n}y_{n}\leq p_{1}x_{1}+p_{2}x_{2}+\ldots+p_{n}x_{n}$). Intuitively, $X$ is directly revealed preferred to $Y$ if $Y$ was affordable at the prices when $X$ was chosen.
\end{definition}

\begin{definition}
We say that $X$ is \textbf{strictly directly revealed preferred (SDRP)} to $Y$ if $P\cdot Y < P\cdot X$ (i.e. $p_{1}y_{1}+p_{2}y_{2}+\ldots+p_{n}y_{n}<p_{1}x_{1}+p_{2}x_{2}+\ldots+p_{n}x_{n}$). Intuitively, $X$ is strictly directly revealed preferred to $Y$ if $Y$ was cheaper than $X$ at the prices when $X$ was chosen.
\end{definition}

\begin{definition}
Consider $k$ bundles, $X_1, X_2, \ldots, X_k$. Then, for $i,j\in\{1,\ldots,k\}$ such that $i\not=j$, $X_i$ is \textbf{indirectly revealed preferred (IRP)} to $X_j$ if there exists some sequence $\{X^m_k\}_{m=1}^{M}$ such that $X^m_k$ DRP $X^{m+1}_k$, $X_i$ DRP $X^1_k$ and $X^M_k$ DRP $X_j$.
\todo{Update to allow for chains of arbitrary length. \checkmark}
\end{definition}

\begin{definition}
We say $X$ is \textbf{revealed preferred (RP)} to $Y$ if $X$ is either directly revealed preferred or indirectly revealed preferred to $Y$.
\end{definition}

\begin{definition}
A set of choice data satisfies the \textbf{Weak Axiom of Revealed Preferences (WARP)} if for all bundles $X$ and $Y$, if $X$ is directly revealed preferred to $Y$, then $Y$ is not directly revealed preferred to $X$.
\end{definition}

\begin{remark}
If a consumer exhibits maximizing behavior, then their set of choice data will satisfy WARP. However, the converse isn't necessarily true when X and Y represent bundles from commodity spaces higher than two dimensions (i.e. there are more than two available goods and thus, $X$, $Y$, and $P$ are n-dimensional vectors such that $n> 2$)  See \citet{Rose1958Consistency-of-Preference:-The-Two-Commodity-Case}. 
\todo{Move citation to bibligraphy instead of footnote. \checkmark}
\end{remark}

\begin{definition}
A set of choice data satisfies the \textbf{Strong Axiom of Revealed Preferences (SARP)} if for all bundles $X$ and $Y$, if $X$ is revealed preferred (directly or indirectly) to $Y$, then $Y$ is not revealed preferred (directly or indirectly) to $X$.
\end{definition}

\begin{remark}
A consumer's set of choice data will satisfy SARP if and only if they exhibit maximizing behavior. See \citet{Samuelson1938A-Note-on-the-Pure-Theory-of-Consumer's-Behaviour}.
\todo{Add citation. \checkmark}
\end{remark}

\begin{definition}
A set of choice data satisfies the \textbf{Weak Generalized Axiom of Revealed Preferences (WGARP)} if for all bundles $X$ and $Y$, if $X$ is directly revealed preferred to $Y$, then $Y$ is not strictly directly revealed preferred to $X$.
\end{definition}

\begin{definition}
A set of choice data satisfies the \textbf{Generalized Axiom of Revealed Preferences (GARP)} if for all bundles $X$ and $Y$, if $X$ is revealed preferred (directly or indirectly) to $Y$, then $Y$ is not strictly directly revealed preferred to $X$.
\end{definition}

\begin{remark}
The key difference between SARP and GARP requirements are that GARP allows consumers to have multiple bundles that maximizes utility under the same price levels (i.e. consumers are allowed to exhibit preferences that treat goods as perfect substitutes of each other).
\todo{Add citation.}
\end{remark}

The following table summarizes each of the axioms and their requirements. For all bundles $X$ and $Y$:

\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{ c|c|c } 
Axiom & If $\ldots$ & Then $\ldots$ \\\hline
WARP&$X$ is \textbf{DRP} to $Y$&$Y$ cannot be \textbf{DRP} to $X$ \\
SARP&$X$ is \textbf{RP} to $Y$&$Y$ cannot be \textbf{RP} to $X$ \\
WGARP&$X$ is \textbf{DRP} to $Y$&$Y$ cannot be \textbf{SDRP} to $X$ \\
GARP&$X$ is \textbf{RP} to $Y$&$Y$ cannot be \textbf{SDRP} to $X$
\end{tabular}
\end{center}
\renewcommand{\arraystretch}{1}

\begin{definition}
The \textbf{Critical Cost Efficiency Index (CCEI)} is the amount by which each budget
constraint must be relaxed in order to remove all violations of GARP. Lower values of CCEI indicates less rational choice data. Note that a CCEI score of 1 indicates a set of choice data satisfies GARP while CCEI approaches 0 as the revealed preferences become less rational. The actual calculations / formula for CCEI will be detailed in the following section.
\end{definition}

\section{Calculating the CCEI (upperbound)}
Suppose for an individual, we have $X_{1}, X_{2}, \ldots, X_{k}$ vectors representing $k$ bundles of $n$ goods chosen at $k$ different times so that $X_{i}\in\mathbb{R}^{n}_{+}$ for all $i\in\{1,2,\ldots,k\}$. Note that the technical notation can get a bit muddy, as $X_{i}=(x_{i1}, x_{i2}, \ldots, x_{in})$ where $x_{ij}$ represents the quantity of good $j$ bought at time $i$. We also have $P_{1}, P_{2}, \dots, P_{k}$ vectors representing $k$ sets of available prices of $n$ goods at $k$ different times so that $P_{i}\in\mathbb{R}^{n}_{++}$ for all $i\in\{1,2,\ldots,k\}$. Similarly, $P_{i}=(p_{i1}, p_{i2}, \ldots, p_{in})$ where $p_{ij}$ represents the price of good $j$ at time $i$.
\todo{Change notation here to have side-by-side subscripts instead of sub-subscripts, ie $p_{ij}$ instead of $p_{i_{j}}$. \checkmark}


We then say $\forall i,j\in\{1,2,\ldots,k\}$ such that $i\not=j$, let:
    $$D_{ij}=\frac{P_{i}X_{j}}{P_{i}X_{i}}-1$$
Note that the above is only negative if and only if $X_{i}$ is strictly directly revealed preferred to $X_{j}$. That's equivalent of saying that although the full bundle of $X_{j}$ was available at the price vector of time $i$, the individual observed chose $X_{i}$ over $X_{j}$.


Then let
$$d_{ij}=max\{D_{ij}, D_{ji}\}$$
We note that $\forall i,j\in\{1,2,\ldots,k\}$ such that $i\not=j$, $d_{ij}<0$ if and only if we have a strictly directly revealed preference conflict. To see this, consider the following:

\begin{itemize}
    \item $D_{ij}<0$ if and only if $X_{i}$ is strictly directly revealed preferred to $X_{j}$ (as per the previous paragraph). 
    \item Similarly, $D_{ji}<0$ if and only if $X_{j}$ is strictly directly revealed preferred to $X_{i}$.
    \item $d_{ij}=max\{D_{ij}, D_{ji}\}<0$ if and only if $D_{ij}<0$ and $D_{ji}<0$, which means that $X_{i}$ is strictly directly revealed preferred to $X_{j}$ \textbf{and} $X_{j}$ is strictly directly revealed preferred to $X_{i}$. 
    \item Thus we have a strictly directly revealed preference conflict if $d_{ij} < 0$.
\end{itemize}

Essentially, $d_{ij}$ perfectly captures whether there is a strictly directly revealed preference conflict between bundles $X_{i}$ and $X_{j}$. Moreover, $d_{ij}$ also captures the degree of the less ``egregious" violation. That is, if $X_{i}$ is way more strictly directly revealed preferred to $X_{j}$ than $X_{j}$ is strictly directly revealed preferred to $X_{i}$, (i.e. if $D_{ij}<<0$ and $D_{ji}<0$), the second preference that caused the violation is less severe and can be more easily "fixed". Since $d_{ij}=max\{D_{ij}, D_{ji}\}<0$, the lesser violation $D_{ji}$ is captured instead.

We then let:

\[
e_{ij}=
\begin{cases}
1 & d_{ij}\geq0 \\
1+d_{ij} & d_{ij}<0
\end{cases}
\]
\todo{Redefine this with piecewise function instead of $\max$ \checkmark}

$e_{ij}$ is called the cross cost efficiency index between bundles $X_{i}$ and $X_{j}$. Note that if $d_{ij}\geq0$, then $-d_{ij}\leq0$ and we would get $e_{ij}=(1-0)=1$. Thus, if there are no strictly directly revealed preference conflicts between $X_{i}$ and $X_{j}$, we would get $e_{ij}=1$. If we have such a violation, then $-d_{ij}>0$ and hence we would get $e_{ij}=1+d_{ij}$ (note that $0<e_{ij}<1$ since $d_{ij}<0$). We claim that, by letting $\hat{ P_iX_i}=e_{ij}\cdot P_iX_i$ and $\hat{P_jX_j}=e_{ij}\cdot P_jX_j$ and using those values to recalculate a $\hat{d_{ij}}$, we would have $\hat{d_{ij}}\geq0$ and hence there are no strictly directly revealed preference violations. This can be seen in the example section and will be proved more rigorously towards the end of this manual. For now, it suffices to note that the lower the value of $e_{ij}$, the harder it is to ``fix" the violation between $X_{i}$ and $X_{j}$.


We then let:
$$e^{*}=\min_{\forall i,j, i\not=j}\{e_{ij}\}$$

$e^{*}$, as given above, would resolve all strictly directly revealed preference violations. The lowest value of $e_{ij}$ for all $i,j\in\{1,2,\ldots,k\}$ would solve the most ``egregious" cross bundle violation and would obviously solve all other remaining cross bundle violations.


\textbf{However, this $\mathbf{e^*}$ is merely an upper bound for the CCEI} as it only resolves cross bundle violations in the dataset. Essentially, $e^*$ would only resolve violations that involve direct revealed preferences. Another value, say $e^*_{2}$, might be needed to resolve indirect revealed preferences and if $e^*_{2}\leq e^*$, then $e^*_{2}$ would be our actual CCEI as it resolve both indirect and direct revealed preference violations (and thus passing GARP).

\section{Examples}

Consider the following data for a single consumer in 3 different time periods:

\begin{center}
\begin{tabular}{ cccccc } 
Choice Number & $p_{1}$ & $p_{2}$ & $x_{1}$ & $x_{2}$ \\
1&1&2&1&2 \\
2&2&1&2&1 \\
3&1&1&2&2
\end{tabular}
\end{center}

Then, $\forall n\in\{1,2,3\}$, we calculate $P_{n}X_{n}$ and $\forall i,j\in\{1,2,3\}$ such that $i\not=j$, we calculate $P_{j}X_{i}$. We then get the following expenditure matrix:

\begin{center}
\begin{tabular}{ cccccc } 
Choice Number & Budget 1 & Budget 2 & Budget 3 \\
1&5&4&3 \\
2&4&5&3 \\
3&6&6&4
\end{tabular}
\end{center}

Looking at the last 3 columns of the matrix, we see that $\forall n\in\{1,2,3\}$, the $(n,n)$ element on the matrix represents the cost of the bundle actually chosen on time period $n$ (i.e. $P_{n}X_{n}$). Additionally, $\forall i,j\in\{1,2,3\}$ such that $i\not=j$, the $(i,j)$ element on the matrix, we calculate the cost of bundle $i$ at available price vector $j$ (i.e. $P_{j}X_{i}$).
\bigskip

From the above expenditure matrix, we can directly check if there are an violations for each of our axioms. Using our definition for directly revealed preferences, we put a $(*)$ next to each cell $(j,i)$ such that $\forall i,j\in\{1,2,3\}$ where $i\not=j$, $P_iX_j\leq P_iX_i$ (i.e. element $(j,i)\leq$ element $(i,i)$). This would indicate bundle $i$ DRP bundle $j$. We then get the following:

\begin{center}
\begin{tabular}{ cccccc } 
Choice Number & Budget 1 & Budget 2 & Budget 3 \\
1&5&4*&3* \\
2&4*&5&3* \\
3&6&6&4
\end{tabular}
\end{center}

Note then that the above fails all our axioms. Since there's a $(*)$ on element $(2,1)$ and element $(1,2)$, indicating that bundle 1 DRP bundle 2 and also bundle 2 DRP bundle 1, failing WARP and SARP. Additionally, if we less our $(*)$ requirement to be: put a $(*)$ next to each cell $(j,i)$ such that $\forall i,j\in\{1,2,3\}$ where $i\not=j$, $P_iX_j<P_iX_i$, we would still get the same matrix and this fail both WGARP and GARP.

\section{Appendix}

Here we prove our claim in \textbf{Section 2}.


We first let:


$$D_{ij}=\frac{P_iX_j}{P_iX_i}-1 \quad \text{and} \quad D_{ji}=\frac{P_jX_i}{P_jX_j}-1.$$



Suppose that $D_{ij}\leq D_{ji}<0$, which means that we have a violation but the strictly directly revealed preference of $X_j$ over $X_i$ is less severe than that of the inverse. Then we take:
$$d_{ij}=max\{D_{ij},D_{ji}\}$$

Note that, by construction, we will simply have:
$$d_{ij}=D_{ji}<0$$

We then can calculate:
$$e_{ij}=1+d_{ij}=1+D_{ji}$$

We then want to prove the following claim:


\textbf{Claim:} $\hat{D_{ji}}=\frac{P_jX_i}{(e_{ij})P_jX_j}-1\geq0$ and hence $\hat{d_{ij}}=max\{\hat{D_{ij}},\hat{D_{ji}}\}\geq0$


\begin{proof}
Recall that:

$$e_{ij}=1+D_{ji}=1+ \left (\frac{P_jX_i}{P_jX_j}-1 \right )=\frac{P_jX_i}{P_jX_j}$$

We then have:

$$\hat{D_{ji}}=\frac{P_jX_i}{(e_{ij})P_jX_j}-1=\frac{P_jX_i}{(\frac{P_jX_i}{P_jX_j})P_jX_j}-1$$

Simplifying:

$$\hat{D_{ji}}=\frac{P_jX_i}{(e_{ij})P_jX_j}-1=\frac{P_jX_i}{P_jX_i}-1=0\geq0$$

Hence, $\hat{D_{ji}}\geq0$ and we have $\hat{d_{ij}}=max\{\hat{D_{ij}},\hat{D_{ji}}\}\geq0$.

\end{proof}

\newpage

\section{Axioms of Revealed Preferences: A Graph Theoretical Approach}

\textbf{THIS FOLLOWING SECTION IS A DRAFT} mainly used to see if there's a faster way to test *ARP and calculate exact values of CCEI without binary search.


Let's begin by looking at the problem with a new perspective. Let $\mathcal{X}=\{X_1,\ldots,X_k\}$ and $\mathcal{P}=\{P_1,\ldots,P_k\}$ be sets of quantity and price vectors, respectively. We then have data set $(\mathcal{X},\mathcal{P})$. Then consider the following directed graphs:


\begin{minipage}{.5\linewidth}
$$G_d=\{\mathcal{X},E_d\}$$
\end{minipage}%
\begin{minipage}{.5\linewidth}
$$G_s=\{\mathcal{X},E_s\}$$
\end{minipage}


where:
$$E_d=\{(X_i,X_j)\ |\ X_i R_D X_j,\ i.e.\ P_iX_i\geq P_iX_j,\ i\not=j\}$$

$$E_s=\{(X_i,X_j)\ |\ X_i R_{SD} X_j,\ i.e.\ P_iX_i> P_iX_j,\ i\not=j\}$$
\todo{Lets make the vertexes just the bundles, not the prices. \checkmark}

We have our vertex set $\mathcal{X}$ for both graphs. Then, it's apparent that $G_d$ is the directed graph for all directly revealed preference relationships and $G_s$ is the direct graph for all \textit{strictly} directly revealed preference relationships. If there's a path / edge from $X_i$ to $X_j$, we have $X_i$ is (strictly) directly revealed preferred to $X_j$ depending on the graph. Further, we shall note that $E_s\subseteq E_d$, as obviously all strictly directly revealed preferred relationships are directly revealed preferred.


We can then characterize our axioms with the following simple definitions:


\textbf{WARP:} If $(\mathcal{X},\mathcal{P})$ passes WARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(X_i,X_j)\in E_d$, then $(X_j, X_i)\not\in E_d$.


\textbf{SARP:} If $(\mathcal{X},\mathcal{P})$ passes SARP, then: $\forall X_i \in \mathcal{X}$, there exists no directed walk from $X_i$ back to itself (I believe this is called a directed cycle, not quite sure on the exact terminology in graph theory).


\textbf{WGARP:} If $(\mathcal{X},\mathcal{P})$ passes WGARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(X_i,X_j)\in E_d$, then $(X_j, X_i)\not\in E_s$.


\textbf{GARP:} If $(\mathcal{X},\mathcal{P})$ passes GARP, then: $\forall i,j\in\{1,\dots,k\}$, if there exists a directed walk from $X_i$ to $X_j$ in $G_d$, then $(X_j,X_i)\not\in E_s$.


\section{WARP/SARP with Graph Theoretical Approach}

Let $A_d$ be the adjacency matrix of $G_d$. To be rigorous, we can generate $A_d$ from our data set in the following manner. First, create the following $(k\times k)$ matrix from $\mathcal{X}$ and $\mathcal{P}$:

\[
Exp = 
 \begin{pmatrix}
  P_1X_1 & P_2X_1 & \cdots & P_kX_1 \\
  P_1X_2 & P_2X_2 & \cdots & P_kX_2 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_k & P_2X_k & \cdots & P_kX_k
 \end{pmatrix}
\]

This above can be referred to as the expenditure matrix. Note that the $(i,j)^{th}$ entry of this matrix represents the cost of bundle $i$ at the price of time $j$ (i.e. $P_jX_i$). Then consider the following:

\[
Exp_{diag} = 
 \begin{pmatrix}
  P_1X_1 & P_2X_2 & \cdots & P_kX_k \\
  P_1X_1 & P_2X_2 & \cdots & P_kX_k \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_1 & P_2X_2 & \cdots & P_kX_k
 \end{pmatrix}
\]

In $Exp_{diag}$, every element in column $j$ is same for all $j$ and represents the actual cost of bundle $j$ at the price vector it was bought at (price vector of time $j$). Then consider the following:

\[
\alpha = Exp-Exp_{diag}
 \begin{pmatrix}
  0 & P_2X_1-P_2X_2 & \cdots & P_kX_1-P_kX_k \\
  P_1X_2-P_1X_1 & 0 & \cdots & P_kX_2-P_kX_k \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_k-P_1X_1 & P_2X_k-P_2X_2 & \cdots & 0
 \end{pmatrix}
\]

Then define a function $f:\mathbb{R}\to\{0,1\}$:

\[ 
f(x)=
    \begin{cases} 
      1 & x\leq0 \\
      0 & x>0
   \end{cases}
\]

We can then get the adjacency matrix by applying $f$ to every element of $\alpha$ except its diagonals and taking its transpose:

\[
A_d =
 \begin{pmatrix}
  0 & f(\alpha_{1,2}) & \cdots & f(\alpha_{1,k}) \\
  f(\alpha_{2,1}) & 0 & \cdots & f(\alpha_{2,k}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  f(\alpha_{k,1}) & f(\alpha_{k,2}) & \cdots & 0
 \end{pmatrix} ^T
\]

We shall briefly explain why this works intuitively:

\begin{itemize}
    \item $f(\alpha_{i,j})=1$ if and only if $\alpha_{i,j}\leq0$
    \item If $\alpha_{i,j}\leq0$, we know that $P_jX_i-P_jX_j\leq0$ and hence $P_jX_i\leq P_jX_j$
    \item Hence, by definition, we have $X_j R_D X_i$
    \item We conclude that $f(\alpha_{i,j})=1$ exactly when $X_j R_D X_i$
    \item However, a value of $1$ in $(i,j)$ position would indicate that a directed edge from vertex $i$ to vertex $j$, but since we want our edges to indicate directly revealed preferences, we want a value of $1$ at the $(j,i)$ positions. Hence, we take the transpose and get $A_{d_{j,i}}=f(\alpha_{i,j})=1$ if and only if $X_j R_D X_i$, which is our desired adjacency matrix.
\end{itemize}

Now we're ready to use matrix operations on $A_d$ to easily test for WARP and SARP.


We know that the number of directed walks from any vertex $i$ to vertex $j$ of length $k$ is given by $A^k_d$ for adjacency matrix $A_d$ (this can be proven by induction). 


\textbf{WARP:} We defined WARP requirements as followed: If $(\mathcal{X},\mathcal{P})$ passes WARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(X_i,X_j)\in E_d$, then $(X_j, X_i)\not\in E_d$. Note that this is true exactly when there \textit{isn't} a directed walk of length 2 from any vertex $i$ back to itself (vertex $i$). Hence we say $(\mathcal{X},\mathcal{P})$ passes WARP if and only if the main diagonal of matrix $A_d^2$ is a vector of 0, i.e.

\[
diag(A_d^2) =
 \begin{pmatrix}
  0 \\
  0 \\
  \vdots \\
  0
 \end{pmatrix}
\]

\textbf{SARP:} We defined SARP requirements as followed: If $(\mathcal{X},\mathcal{P})$ passes SARP, then: $\forall X_i \in \mathcal{X}$, there exists no directed walk from $X_i$ back to itself. So we have to account for walks of \textit{any length}. Consider the following:

$$B_d=\sum_{i=2}^{k}A_d^i$$

$B_d$'s $(i,j)^{th}$ element the number of directed walks of at least length 2 (since the maximum length of a walk is $k$ without looping around again, the cardinality of the vertex set) from vertex $i$ to vertex $j$. The values of the main diagonal is then the number of directed walks of any length (at least 2) from vertex $i$ back to itself. Then $(\mathcal{X},\mathcal{P})$ passes SARP if and only if the main diagonal of matrix $B_d=\sum_{i=2}^{k}A_d^i$ is a vector of 0, i.e.:

\[
diag(B_d) = diag(\sum_{i=2}^{k}A_d^i)=
 \begin{pmatrix}
  0 \\
  0 \\
  \vdots \\
  0
 \end{pmatrix}
\]

\section{WGARP/GARP with Graph Theoretical Approach}

To test for WGARP and GARP, we would need a second adjacency matrix to reflect $G_s$. Let's call it $A_s$. Since the edge set $E_s$ is pretty similar (in fact a subset of) to $E_d$, we just have to make a slight adjustment to $A_d$'s construction. First we take $\alpha$ as before, since our expenditure matrix (which represents the same vertex set) doesn't change. Then we define a function $g:\mathbb{R}\to\{0,1\}$:

\[ 
g(x)=
    \begin{cases} 
      1 & x<0 \\
      0 & x\geq0
   \end{cases}
\]

Then we can define $A_s$ as followed:

\[
A_s =
 \begin{pmatrix}
  0 & g(\alpha_{1,2}) & \cdots & g(\alpha_{1,k}) \\
  g(\alpha_{2,1}) & 0 & \cdots & g(\alpha_{2,k}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  g(\alpha_{k,1}) & g(\alpha_{k,2}) & \cdots & 0
 \end{pmatrix} ^T
\]

The reasoning behind how $A_s$ works is the same as the rationale behind $A_d$ except the conditions of $g$ are set such that the relationships between bundle $i$ and bundle $j$ need to be SDRP in order for $(i,j)$ to be 1. Then the following are ways in which we can test for WGARP and GARP.


\textbf{WGARP:} We can test for WGARP by multiplying $A_d$ and $A_s$ by matrix multiplication.


\textit{Theorem 1:} A data set $(\mathcal{X}, \mathcal{P})$ passes WGARP if and only if the main diagonal of $A_d\times A_s$ is a vector of 0.

\begin{proof}
Recall that: If $(\mathcal{X},\mathcal{P})$ passes WGARP, then: $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(X_i,X_j)\in E_d$, then $(X_j, X_i)\not\in E_s$. Which, when put in context of adjacency matrices, means that $\forall i,j\in\{1,\dots,k\}$ where $i\not=j$, if $(i,j)^{th}$ position of $A_d$ ($A_{d_{i,j}}$) is equal to one, then the $(j,i)^{th}$ position of $A_s$ ($A_{s_{i,j}}$) cannot be equal to 1. We then consider $A_d\times A_s$ and its main diagonal. 


$\forall i\in\{1,\ldots,k\}$, let $\delta_i$ be the vector of the $i^{th}$ row of $A_d$ and $\xi_i$ be the vector of the $i^{th}$ column of $A_s$. Then we have $(A_d\times A_s)_{i,i}=\delta_i\cdot\xi_i$. Note that $\delta_{i_i}=0$ and $\xi_{i_i}=0$ since originally $A_d$ and $A_s$ have all 0's across their main diagonals.


We then know that, $\forall i\in\{1,\ldots,k\}$, $(A_d\times A_s)_{i,i}=0$ (i.e. the main diagonal of $A_d\times A_s$ is a zero vector) if and only if $\delta_i\cdot\xi_i=0$. Further, $\delta_i\cdot\xi_i=0$ if and only if $\forall j \in\{1,\ldots,k\}$ such that $i\not=j$, $\delta_{i_j}\cdot \xi_{i_j}=0$ (since we know that $\delta_{i_i}=\xi_{i_i}=0$, they won't contributed to the overall sum, which is what we want anyways by the WGARP rules). We then know that $\delta_{i_j}\cdot \xi_{i_j}=0$ if and only if $\delta_{i_j}=0$ or $\xi_{i_j}=0$. Further, "$\delta_{i_j}=0$ or $\xi_{i_j}=0$" if and only if $\neg (\delta_{i_j}=\xi_{i_j}=1)$ (since all terms in $A_d$ and $A_s$ are either one or zero). Note that $\delta_{i_j}=A_{d_{i,j}}$ and $\xi_{i_j}=A_{s_{j,i}}$ (since $\xi_i$ is the vector of the $i^{th}$ \textit{column} of $A_s$). Hence, $\neg (\delta_{i_j}=\xi_{i_j}=1)$ if and only if $\neg (A_{d_{i,j}}=A_{s_{j,i}}=1)$, which happens if $A_{d_{i,j}}=1$, then $A_{s_{j,i}}=0\not=1$. Note that this is the exact definition of passing WGARP. Summarizing this, we have:
\begin{flalign*}
\forall i\in\{1,\ldots,k\}, (A_d\times A_s)_{i,i}=0 &\iff \delta_i\cdot\xi_i=0 &\\
&\iff \forall j \in\{1,\ldots,k\}\textrm{, }i\not=j,\ \delta_{i_j}\cdot \xi_{i_j}=0 &\\
&\iff \delta_{i_j}=0\textrm{ or }\xi_{i_j}=0 &\\
&\iff \neg (\delta_{i_j}=\xi_{i_j}=1) &\\
&\iff \neg (A_{d_{i,j}}=A_{s_{j,i}}=1) &\\
&\iff (A_{d_{i,j}}=1 \implies A_{s_{j,i}}=0\not=1)
\end{flalign*}

Hence, $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(A_d\times A_s)_{i,i}=0 \iff (A_{d_{i,j}}=1 \implies A_{s_{j,i}}=0\not=1)$. Note the first statement simply means the main diagonal of $A_d\times A_s$ is the zero vector and the second statement is the condition for passing WGARP. We then have "passing WGARP" $\iff$ "main diagonal of $A_d\times A_s$ is a zero vector". Thus our claim is proven (since $A\iff B$ is the same as $B\iff A$).
\end{proof}

\textbf{GARP:} We can test for GARP by finding multiplying $\sum_{i=1}^{k-1}A_{d}^{i}$ with $A_s$ by matrix multiplication.


Before we begin, consider the following lemma:


\textit{Lemma A:} Let $X_1, X_2, Y$ be $(m\times m)$ matrices such that:

\begin{minipage}{.5\linewidth}
\[
X_1 =
 \begin{pmatrix}
  x_{1,1} & x_{1,2} & \cdots & x_{1,m} \\
  x_{2,1} & x_{2,2} & \cdots & x_{2,m} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{m,1} & x_{m,2} & \cdots & x_{m,m}
 \end{pmatrix}
\]
\end{minipage}
\begin{minipage}{.5\linewidth}
\[
X_2 =
 \begin{pmatrix}
  0 & x_{1,2} & \cdots & x_{1,m} \\
  x_{2,1} & 0 & \cdots & x_{2,m} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{m,1} & x_{m,2} & \cdots & 0
 \end{pmatrix}
\]
\end{minipage}
and 

\[
Y =
 \begin{pmatrix}
  0 & y_{1,2} & \cdots & y_{1,m} \\
  y_{2,1} & 0 & \cdots & y_{2,m} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  y_{m,1} & y_{m,2} & \cdots & 0
 \end{pmatrix}
\]

Then the main diagonals of $X_1\times Y$ and $X_2\times Y$ are the same (denoted $diag(X_1\times Y)=diag(X_2\times Y)$).

\begin{proof}
Fix an arbitrary $i\in\{1,\ldots,m\}$. Then let $Z_1=X_1\times Y$ and $Z_2=X_2\times Y$. We want to show that $Z_{1_{i,i}}=Z_{2_{i,i}}$. Note that $Z_{1_{i,i}}=x_{i,1}y_{1,i}+x_{i,2}y_{2,i}+\ldots+x_{i,i}y_{i,i}+\ldots+x_{i,m}y_{m,i}$ and that $Z_{2_{i,i}}=x_{i,1}y_{1,i}+x_{i,2}y_{2,i}+\ldots+0y_{i,i}+\ldots+x_{i,m}y_{m,i}$. The only difference between those are that for the first equation, we have $x_{i,i}y_{i,i}$ as the $i^{th}$ term and the second equation, we have $0y_{i,i}$ as the $i^{th}$ term. However, since $y_{i,i}=0$, we have that $x_{i,i}y_{i,i}=0y_{i,i}=0$ and thus $Z_{1_{i,i}}=Z_{2_{i,i}}$.
\end{proof}

\textit{Theorem 2:} A data set $(\mathcal{X},\mathcal{P})$ passes GARP if and only if the main diagonal of $(\sum_{i=1}^{k-1}A_{d}^{i})\times A_s$ is a vector of 0.

\begin{proof}
The proof of this will look very similar to the proof of WGARP, except that the elements in $\sum_{i=1}^{k-1}A_{d}^{i}$ will no longer be binary $(\{0,1\})$. However, they are all $\geq0$. Let $\beta_d=\sum_{i=1}^{k-1}A_{d}^{i}$, then $\beta_{d_{i,j}}$ would be the number of directed walks between vertex $i$ and vertex $j$ of any length up to $k-1$. Note that we chose to sum up to $k-1$ since anything past that, we would definitely have a cycle and summing past that point is redundant.


We want to make sure that if there's a directed walk from vertex $i$ to vertex $j$ in $G_d$, there isn't a directed edge between vertex $j$ and vertex $i$ in $G_s$. Formally, this is just our definition for GARP: If $(\mathcal{X},\mathcal{P})$ passes GARP, then: $\forall i,j\in\{1,\dots,k\}$, if there exists a directed walk from $X_i$ to $X_j$ in $G_d$, then $(X_j,X_i)\not\in E_s$. This is similar to WGARP in the sense that we're simply replacing the directed edge in the "if" statement to a directed walk. Note then that we have $\beta_d=\sum_{i=1}^{k-1}A_{d}^{i}$, which captures the existence of any directed walks between any two verices. Hence if $\beta_{d_{i,j}}>0$, there exists at least 1 directed walk between vertex $i$ and vertex $j$ on graph $G_d$. We can then use our proof for WGARP and \textit{Lemma A} to construct the rest of this proof.


In our proof of WGARP, we see that $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(A_d\times A_s)_{i,i}=0 \iff (A_{d_{i,j}}=1 \implies A_{s_{j,i}}=0\not=1)$. There are two changes we need to make to this statement to fit our GARP claim. 

First, we see that we're no longer working with $A_d$ but working with $\beta_d$, which represents directed walks instead of edges. $\beta_d$ is also no longer filled with binary elements $\in\{0,1\}$. We can tweak our statement to fit these conditions by stating: $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(\beta_d\times A_s)_{i,i}=0 \iff (\beta_{d_{i,j}}>0 \implies A_{s_{j,i}}=0)$. Now, $\beta_{d_{i,j}}>0$ simply means there exists at least one path from $i,j$ regardless of length, which is what we want.

Second, when we constructed our proof of WGARP, we noted that "since we know that $\delta_{i_i}=\xi_{i_i}=0$ (i.e. $A_{d_{i,i}}=0$ and $A_{s_{i,i}}=0$), they won't contributed to the overall sum, which is what we want anyways by the WGARP rules". However, this no longer holds as we could have $\beta_{d_{i,i}}\not=0$ for some $i$ if there's any directed cycles in $G_d$ (but we don't care about those in GARP, per the definition of the axiom). But we do still have $A_{s_{i,i}}=0$ for all $i$ and thus, by \textit{Lemma A}, we don't need to "clear-out" the diagonals of $\beta_d$ before multiplying. Hence, our statement above still holds. 

Thus, we have: $\forall i, j\in\{1,\ldots,k\}$ such that $i\not=j$, $(\beta_d\times A_s)_{i,i}=0 \iff (\beta_{d_{i,j}}>0 \implies A_{s_{j,i}}=0)$. The first statement simply means the main diagonal of $(\beta_d=\sum_{i=1}^{k-1}A_{d}^{i}\times A_s)$ is a zero vector and the second statement is the condition for passing GARP, hence our proof is complete (since $A\iff B$ is the same as $B\iff A$).
\end{proof}

\section{CCEI with Graph Theoretical Approach}

The following section explores a method to use graph theory and weighted adjacency matrices to obtain a CCEI score for each consumer. 
\bigskip

\textbf{Idea:} The main idea here is that we can reconsider our graphs $G_d, G_s$ to make them into weighted graphs that reflect the extent to which a bundle is (strictly) directly revealed preferred to another one. We then use the information obtained from these new graphs to calculate a CCEI score associated with our dataset $(\mathcal{X},\mathcal{P})$.
\bigskip

Let $\mathcal{X}=\{X_1,\ldots,X_k\}$ and $\mathcal{P}=\{P_1,\ldots,P_k\}$ as before. Then let us set up $G_d$ and $G_s$ but define functions: $W_d:E_d \to\mathbb{R}_{\geq1}$ and $W_s:E_s \to\mathbb{R}_{>1}$ so that $W(e_d)$ is the weight of edge $e_d$ and $W(e_s)$ is the weight of edge $e_s$. Let us rigorously define our weight functions.
\bigskip

Let $e_d \in E_d$ and $e_s\in E_s$ Then we know that, for $i\not=j$, $e_d=(X_i, X_j)$ such that $X_i R_D X_j$, which means $P_iX_i\geq P_iX_j$. We define $W_d$ such that $W_d(e_d)=\frac{P_iX_i}{P_iX_j}$. Further, let $e_s\in E_s$, we then know that, for $i\not=j$, $e_s=(X_i, X_j)$ such that $X_i R_{SD} X_j$, which means $P_iX_i>P_iX_j$. Then $W_s(e_s)=\frac{P_iX_i}{P_iX_j}>1$.
\bigskip

Then we create the weighted adjacency matrices (for $G_d$ and $G_s$) from our dataset. Let's consider our expenditure matrix, as given in the previous section:

\[
Exp = 
 \begin{pmatrix}
  P_1X_1 & P_2X_1 & \cdots & P_kX_1 \\
  P_1X_2 & P_2X_2 & \cdots & P_kX_2 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_k & P_2X_k & \cdots & P_kX_k
 \end{pmatrix}
\]

Similarly, we also retrieve a second matrix, $Exp_{diag}$ from it:

\[
Exp_{diag} = 
 \begin{pmatrix}
  P_1X_1 & P_2X_2 & \cdots & P_kX_k \\
  P_1X_1 & P_2X_2 & \cdots & P_kX_k \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_1X_1 & P_2X_2 & \cdots & P_kX_k
 \end{pmatrix}
\]

We then create a matrix $Q$ such that:

\[
Q = Exp_{diag} \varoslash Exp
\]

Since the "$\varoslash$" means division by element, and we know that both $Exp_{diag}, Exp$ are both positive matrices with no zero element (prices are clearly always positive, and all we need is that the quantity vector has at least one non-zero element in it, which we can assume), we get the following:

\[
Q = 
 \begin{pmatrix}
  1 & q_{1,2} & \cdots & q_{1,k} \\
  q_{2,1} & 1 & \cdots & q_{2,k} \\
  \vdots & \vdots & \ddots & \vdots \\
  q_{k,1} & q_{k,2} & \cdots & 1
 \end{pmatrix} =
 \begin{pmatrix}
 1 & \frac{P_2X_2}{P_2X_1} & \cdots & \frac{P_kX_k}{P_kX_1} \\[6pt]
 \frac{P_1X_1}{P_1X_2} & 1 & \cdots & \frac{P_kX_k}{P_kX_2} \\[6pt]
 \vdots & \vdots & \ddots & \vdots \\[6pt]
 \frac{P_1X_1}{P_1X_k} & \frac{P_2X_2}{P_2X_k} & \cdots & 1
 \end{pmatrix}
\]

Then we define two functions, $d:\mathbb{R}_{+}\to\mathbb{R}_{\geq1}\cup\{0\}$ and $s:\mathbb{R}_{+}\to\mathbb{R}_{>1}\cup\{0\}$, as follows:

\begin{minipage}{.5\linewidth}
\[ 
d(x)=
 \begin{cases} 
   x & x\geq1 \\
    0 & x<1
 \end{cases}
\]
\end{minipage}
\begin{minipage}{.5\linewidth}
\[
s(x)=
    \begin{cases} 
      x & x>1 \\
      0 & x\leq1
   \end{cases}
\]
\end{minipage}

Note that $d$ and $s$ are functions that will be applied to $Q$ for it to reflect the extent of either direct and strictly direct revealed preferences, with 0 being non-(strictly) directly revealed preferred. We then can define our adjacency matrices, $\Omega_d$ and $\Omega_s$ as follows (by applying $d,s$ to each element excpept the diagonals, and setting the diagonals to 0, as there are no loops in our graph):

\[
\Omega_d =
  \begin{pmatrix}
  0 & d(q_{1,2}) & \cdots & d(q_{1,k}) \\
    d(q_{2,1}) & 0 & \cdots & d(q_{2,k}) \\
    \vdots & \vdots & \ddots & \vdots \\
    d(q_{k,1}) & d(q_{k,2}) & \cdots & 0
  \end{pmatrix}^T
\]

\[
\Omega_s =
  \begin{pmatrix}
  0 & s(q_{1,2}) & \cdots & s(q_{1,k}) \\
    s(q_{2,1}) & 0 & \cdots & s(q_{2,k}) \\
    \vdots & \vdots & \ddots & \vdots \\
    s(q_{k,1}) & s(q_{k,2}) & \cdots & 0
  \end{pmatrix} ^T
\]

Note that these are awfully similar to $A_d$ and $A_s$ in the previous section except that the matrix is no longer filled with binary elements but actually capture the extent of each preference. We shall briefly go over the reasoning behind why $\Omega_d$ is in fact the weighted adjacency matrix. The reader should follow similar logic to understand $\Omega_s$ as well. Let $i\not=j$ for simplicity and:
\begin{itemize}
  \item We first see that $\Omega_{d_{i,j}}$ is in fact $d(q_{j,i})$.
  \item If $\Omega_{d_{i,j}}=d(q_{j,i})=0$, we have $q_{j,i}<1$ and hence $\frac{P_iX_i}{P_iX_j}<1$ and thus $P_iX_i<P_iX_j$. We then get $\neg(X_i R_D X_j)$ so there's no edge between $X_i$ and $X_j$. Hence $\Omega_{d_{i,j}}=-1$, as desired.
  \item If $\Omega_{d_{i,j}}=d(q_{j,i})\geq1$, we then have $q_{j,i}\geq1$ and hence $\frac{P_iX_i}{P_iX_j}\geq1$ and thus $P_iX_i\geq P_iX_j$. Thus we have an edge between $X_i$ and $X_j$ for $i\not=j$, as $X_iR_D X_j$ by definition. We then note that $d$ is in fact a bijective function (identity function) for the subset of the domain that's $\geq1$ and that since $q_{j,i}\geq1$, we have that $d(q_{j,i})=q_{j,i}$ Following definitions, we get $\Omega_{d_{i,j}}=d(q_{j,i})=q_{j,i}=\frac{P_iX_i}{P_iX_j}\geq1$, which are precisely the weights we wanted with the $W_d$ function defined earlier.
\end{itemize}
Let $X_i$ and $X_j$ be two vertices (bundles) for $i,j\in\{1,\ldots,k\}$ such that $i\not=j$. If $(X_i, X_j)\in E_d$ (i.e. $X_i$ is directly revealed preferred to $X_j$), we then know that, for $e_d=(X_i, X_j)\in E_d$, $W_d(e_d)=\frac{P_iX_i}{P_iX_j}\geq1$. Then, we claim that $\exists c_{ij}\in(0,1]$ such that $c_{ij}\frac{P_iX_i}{P_iX_j}=1$. Intuitively, we say that for some bundle $i$ that we know is at least directly revealed preferred (and thus have to be SDRP, by definition) to bundle $j$, we can find some $c_{ij}$ that "shrinks" the budget of $i$ by an amount such that bundle $i$ is no longer strictly directly revealed preferred to bundle $j$ (but still directly revealed preferred). Putting in graph theory context, we're shrinking the price vector of the numerator by a constant before "multiplying" it to the quantity vector, therefore changing the relationship between vertices $X_i$ and $X_j$. We can see this easily by rearranging our equation with vector operation rules: $\frac{(c_{ij}P_i)X_i}{P_iX_j}=1$. Now we shall prove our claim of $c_{ij}$'s existence.



\bigskip

\textit{Lemma B:} For $i,j\in\{1,\ldots,k\}$ such that $i\not=j$, let $e_d=(X_i,X_j)\in E_d$ and $W_d(e_d)=\frac{P_iX_i}{P_iX_j}\geq1$ be the weight of $e_d$ as defined. Then $c_{ij}=\frac{1}{W_d(e_d)}=\frac{P_iX_j}{P_iX_i}$ is the constant such that $c_{ij}\frac{P_iX_i}{P_iX_j}=1$.

This is pretty self explanatory. We see that, since $W_d(e_d)\geq1$ by construction, thus $c_{ij}=\frac{1}{W_d(e_d)}\in(0,1]$. It's interesting to note that $s(c_{ij}\frac{P_iX_i}{P_iX_j})=0$ by the definition of function $s$.

\textbf{What does this buy us?}

Note that for all edges, $W\big((X_i, X_j)\big)$ is the $(i,j)^{th}$ element of the adjacency matrix in which the edge exists. In this case, if $(X_i, X_j)$ is an edge causing a violation, we can ``solve'' the violation with a constant $c_{ij}$.
\bigskip

We now define function $\lambda:\mathbb{R}^{m\times m} \times \mathbb{R}^{m\times m} \to \mathbb{R}^{m\times m}$ as follows:

Let $X,Y$ be $(m\times m)$ matrices and let $\lambda(X,Y)=Z$, then, $\forall i,j\in\{1,\ldots,m\}$:
$$Z_{i,j}=max\{min\{X_{i,1}, Y_{1,j}\}, min\{X_{i,2}, Y_{2,j}\}, \ldots, min\{X_{i,m}, Y_{m,j}\}\}$$
where $z_{i,j}$ is the $(i,j)^{th}$ element of $Z$. note that this is similar with matrix multiplication in that we consider the $i^{th}$ row of $X$ and $j^{th}$ column of $Y$, but instead of pairwise multiplication and overall summation (i.e. dot product), we take pairwise minimum and overall maximum. 
\bigskip

We further define a function $\lambda_t:(\mathbb{R}^{m\times m})^t\to\mathbb{R}^{m\times m}$ such that for $Y_1,\ldots,Y_t\in \mathbb{R}^{m\times m}$, we have:



\[
  \lambda_t(Y_1,\ldots, Y_t) =
    \underbrace{\lambda(\cdots\lambda(\lambda(\lambda(}_\text{t-1 times}
    Y_1,Y_2),Y_3),Y_4),\ldots, Y_t)
\]

\textit{Theorem 3:} $\forall t\in \mathbb{N}_{\geq2}$,
\[
  \lambda_t(\underbrace{\Omega_d,\ldots,\Omega_d}_\text{t times})_{i,j}=
  \begin{cases}
  0 & \textrm{if } X_i, X_j\textrm{ are not connected by a walk of }t \textrm{ edges} \\
  & \textrm{on }G_d \\
  \rho_{i,j} & \textrm{such that }\rho_{i,j}\textrm{ is the weight of the strongest edge } \\
  &\textrm{among the weakest edges of all walks of } t \textrm{ edges} \\
  & \textrm{from }X_i\textrm{ to }X_j \textrm{ on } G_d
  \end{cases}
\]
We abbreviate $\lambda_t(\Omega_d,\ldots,\Omega_d)=\lambda^t(\Omega_d)$. Then, we formally write:
\[
  \lambda^t(\Omega_d)_{i,j}=
  \begin{cases}
  0 & \textrm{if } \nexists (X^v)_{v=1}^{t-1} \textrm{ such that } \forall v,\ X^v\in\mathcal{X}\textrm{ and } \\
  & (X^v,X^{v+1})\in E_d, (X_i,X^1)\in E_d \textrm{ and } \\
  & (X^{t-1},X_j)\in E_d\\ 
  \rho_{i,j} & \textrm{if }\exists (X^{uv})_{u,v=1}^{u=U,\ v=t-1}\ (\textrm{for some }U\geq1) \\
  & \textrm{such that }\forall u,\ \forall v,\ X^{uv}\in\mathcal{X} \textrm{ and } \\
  & (X^{uv},X^{u(v+1)})\in E_d\ (\textrm{if }t>2), (X_i, X^{u1})\in E_d \textrm{ and } \\
  & (X^{u(t-1)},X_j)\in E_d, \textrm{ then: } \\
  & \rho_{i,j}=max\Big\{min\big\{W_d\big((X_i,X^{u1})\big),W_d\big((X^{uv},X^{u(v+1)})\big), \\
  & W_d\big((X^{u(t-1)},X_j)\big)\ |\ v\in\{1,\ldots,t-2\}\big\}\ |\ u\in\{1,\ldots,U\}\Big\} \\
  &\textrm{if } t>2,\ \textrm{and }\\
  &\rho_{i,j}=max\Big\{min\big\{W_d\big((X_i,X^{u1})\big), W\big((X^{u1},X_j)\big)\big\}\ |\ u\in\\
  &\{1,\ldots,U\}\Big\} \textrm{ if } t=2\\
  \end{cases}
\]
Note that $\forall i,j$, $\rho_{i,j}\geq1$ since $W_d$ is the weight function as defined earlier such that $W:E_d \cup E_s\to\mathbb{R}_{\geq0}$. Further, intuitively, we say:
\[
  \lambda^t(\Omega_d)_{i,j}=
  \begin{cases}
  0 & \textrm{if bundles }X_i \textrm{ and }X_j\textrm{ are not connected by a} \\
  & \textrm{preference chain of length } t\\
  \rho_{i,j} & \textrm{where }\rho_{i,j}\textrm{ is the largest weight across the weights} \\
  & \textrm{of the smallest preferences in each preference chain }\\
  & \textrm{of length }t
  \end{cases}
\]

Before we begin the proof of \textit{Theorem 3}, let's take a look at the following lemma.

\textit{Lemma C:} We claim that, for any arbitrary real values of $\alpha^j_i$ and $\beta$, the following equality holds:
\[
min
\left\{
  \begin{aligned}
    max\left\{
      \begin{aligned}
        min\{\alpha_1^1,&\ldots,\alpha_1^J\} \\
        &\vdots \\
        min\{\alpha_I^1,&\ldots,\alpha_I^J\}
      \end{aligned}
    \right\}
  \end{aligned}
  , \beta
\right\}
=
max\left\{
  \begin{aligned}
    min\{\alpha_1^1,&\ldots,\alpha_1^J,\beta\} \\
    &\vdots \\
    min\{\alpha_I^1,&\ldots,\alpha_I^J,\beta\} \\
  \end{aligned}
\right\}
\]

\begin{proof}
Considering the LHS, we have that:
\[
LHS=
\begin{cases}
\beta & \textrm{if } \beta\leq
max\left\{
      \begin{aligned}
        min\{\alpha_1^1,&\ldots,\alpha_1^J\} \\
        &\vdots \\
        min\{\alpha_I^1,&\ldots,\alpha_I^J\}
      \end{aligned}
    \right\} \\
    \\
\alpha_x^y\not=\beta &\textrm {if } \alpha_x^y\leq\alpha_x^1,\ldots,\alpha_x^J \textrm{ and} \\
& \alpha_x^y\geq min\{\alpha_1^1,\ldots,\alpha_1^J\},\ldots,min\{\alpha_I^1,\ldots,\alpha_I^J\} \textrm { and}\\
& \alpha_x^y\leq \beta
\end{cases}
\]
We can further write this as:
\[
LHS=
\begin{cases}
\beta & \textrm{if }\exists\ i_1\in\{1,\ldots,I\} \textrm{ such that }\forall j\in\{1,\ldots,J\}, \beta\leq\alpha_{i_1}^j \\
\alpha_x^y\not=\beta & \textrm{if } \alpha_x^y\leq\alpha_x^j,\ \forall j\in\{1,\ldots,J\} \textrm{ and} \\
& \forall i\in\{1,\ldots,I\},\ \exists\ j\in\{1,\ldots,J\} \textrm{ such that } \alpha_x^y\geq\alpha_i^j \textrm{ and } \\
& \alpha_x^y\leq \beta
\end{cases}
\]
Now consider the RHS, we have:
\[
RHS = 
\begin{cases}
\beta & \textrm{if }\exists\ i_1\in\{1,\ldots,I\} \textrm{ such that }\forall j\in\{1,\ldots,J\}, \beta\leq\alpha_{i_1}^j\ (*) \\
& \textrm{i.e. } \beta\geq\alpha_{i_1}^1,\ldots,\alpha_{i_1}^J\\
\alpha_x^y\not=\beta &\textrm{if }\alpha_x^y\leq\beta\ (***) \textrm{ and}\\
& \alpha_x^y\leq\alpha_x^1,\ldots,\alpha_x^J \textrm{ and}\\
& \alpha_x^y\geq min\{\alpha_1^1,\ldots,\alpha_1^J,\beta\},\ldots,min\{\alpha_I^1,\ldots,\alpha_I^J,\beta\}\ (**)
\end{cases}
\]
Looking at $(*)$, the condition ``$\exists\ i_1\in\{1,\ldots,I\} \textrm{ such that }\forall j\in\{1,\ldots,J\}, \beta\leq\alpha_{i_1}^j$'' suffices because under such condition, $RHS=max\{\beta, \alpha_{i_2}\ |\ i_2\in I_2\}$ where $I_2\subseteq I$ is the set of $i\in I$ such that $\forall i_2\in I_2$, $min\{\alpha_{i_2}^1,\ldots,\alpha_{i_2}^J,\beta\}=\alpha_{i_2}\not=\beta$. By the definition of the minimum, we then have: $\alpha_{i_2}\leq\beta$ and hence, $RHS=max\{\beta, \alpha_{i_2}\leq\beta\ |\ i_2\in I_2\}=\beta$, as desired.

We then look at $(**)$: $\alpha_x^y\geq min\{\alpha_1^1,\ldots,\alpha_1^J,\beta\},\ldots,min\{\alpha_I^1,\ldots,\alpha_I^J,\beta\}$. We can rewrite this condition as: $\alpha_x^y\geq min\big\{min\{\alpha_1^1,\ldots,\alpha_1^J\},\beta\big\},\ldots,min\big\{min\{\alpha_I^1,\\\ldots,\alpha_I^J\},\beta\big\}$. We then claim that, given the conditions of $(***)$, it suffices to write: $\alpha_x^y\geq min\big\{min\{\alpha_1^1,\ldots,\alpha_1^J\}\big\},\ldots,min\big\{min\{\alpha_I^1,\ldots,\alpha_I^J\}\big\}$ and hence $\alpha_x^y\geq min\{\alpha_1^1,\ldots,\alpha_1^J\},\ldots,min\{\alpha_I^1,\ldots,\alpha_I^J\}$. The following is a brief proof:

Suppose towards a contradiction that the condition written above does not suffice and the entire condition, $\alpha_x^y\geq min\big\{min\{\alpha_1^1,\ldots,\alpha_1^J\},\beta\big\},\ldots,min\big\{min\{\alpha_I^1,\\\ldots,\alpha_I^J\},\beta\big\}$, is required. Then, we know that $\exists i\in\{1,\ldots,I\}$ such that $min\big\{\\min\{\alpha_i^1,\ldots,\alpha_i^J\},\beta\big\}=\beta$. We then have that $\alpha_x^y\geq\beta$, but by $(***)$, we also have $\alpha_x^y\leq\beta$, hence we have $\alpha_x^y=\beta$, which is a direct contradiction to the initial condition of $\alpha_x^y\not=\beta$. Hence, we know that it suffices to write $\alpha_x^y\geq min\{\alpha_1^1,\ldots,\alpha_1^J,\beta\},\ldots,min\{\alpha_I^1,\ldots,\alpha_I^J,\beta\}$ as $\alpha_x^y\geq min\{\alpha_1^1,\ldots,\alpha_1^J\},\ldots,min\{\\ \alpha_I^1,\ldots,\alpha_I^J\}$, given $(***)$.

Putting this all together, we have the following for the $RHS$:
\[
RHS=
\begin{cases}
\beta & \textrm{if }\exists\ i_1\in\{1,\ldots,I\} \textrm{ such that }\forall j\in\{1,\ldots,J\}, \beta\leq\alpha_{i_1}^j \\
\alpha_x^y\not=\beta & \textrm{if } \alpha\leq\beta \textrm{ and}\\
& \forall i\in\{1,\ldots,I\}, \exists\ j\in\{1,\ldots,J\}, \textrm{ such that } \alpha_x^y\geq\alpha_i^j \textrm{, and}\\
& \alpha_x^y\leq\alpha_x^j,\ \forall j\in\{1,\ldots,J\}
\end{cases}
\]

Note then that $LHS=RHS$ and hence our lemma is proven.
\end{proof}

Now we shall prove \textit{Theorem 3}.

\begin{proof}
We approach this with proof by induction.

\textbf{Base Case:} Let $t=2$, we then have $\lambda_2(\Omega_d,\Omega_d)=\lambda(\Omega_d,\Omega_d)$. Set arbitrary $i,j\in\{1,\ldots,k\}$ and consider:
\begin{flalign*}
\lambda(\Omega_d,\Omega_d)_{i,j} &=max\big\{min\{\Omega^d_{i,1},\Omega^d_{1,j}\},\ldots,min\{\Omega^d_{i,k},\Omega^d_{k,j}\}\big\} &\\
& =max\big\{min\{\Omega^d_{i,f},\Omega^d_{f,j}\}\ |\ f\in\{1,\ldots,k\}\big\}
\end{flalign*}
We then consider two cases:

\circled{1} If $\forall f\in\{1,\ldots,k\}$, $\Omega^d_{i,f}=0$ or $\Omega^d_{f,j}=0$, then $\lambda(\Omega_d,\Omega_d)_{i,j}=0$ as $\forall i,j\in\{1,\ldots,k\}$, $\Omega^d_{i,j}\in\{0\}\cup\mathbb{R}_{\geq1}$. Thus, if $\forall f\in\{1,\ldots,k\}$, $\Omega^d_{i,f}=0$ or $\Omega^d_{f,j}=0$, then $\lambda(\Omega_d,\Omega_d)_{i,j}=max\{0,\ldots,0\}=0$.

\circled{2} Suppose $\exists F\subseteq\{1,\ldots,k\}$ (where $F\not=\varnothing$) such that $\forall f\in F$, $\Omega^d_{i,f}\geq1$ and $\Omega^d_{f,j}\geq1$. We then have $\lambda(\Omega_d,\Omega_d)_{i,j}=max\big\{min\{\Omega^d_{i,f},\Omega^d_{f,j}\}, 0\ |\ f\in F\big\}=max\big\{min\{\Omega^d_{i,f},\Omega^d_{f,j}\}\ |\ f\in F\big\}$.

Considering \circled{1} and \circled{2}, we have precisely the definition in our claim:

For \circled{1}, if $\nexists f$ such that $\Omega^d_{i,f}\geq1$ and $\Omega^d_{f,j}\geq1$, then we know $(X_i,X_f)\not\in E_d$ or $(X_f, X_j)\not\in E_d$ for all $f\in\{1,\ldots,k\}$. Hence, $\nexists (X^v)_{v=1}^1=X^v$ such that $X^v\in \mathcal{X}$ and $(X_i,X^v)\in E_d$ and $(X^v,X_j)\in E_d$, which is precisely our condition in the first case of the formalized version of our claim.

For \circled{2}, we have that $\lambda(\Omega_d,\Omega_d)_{i,j}=max\big\{min\{\Omega^d_{i,f},\Omega^d_{f,j}\}\ |\ f\in F\big\}$, where $\varnothing\not=F\subseteq\{1,\ldots,k\}$ such that $\forall f\in F$, $\Omega^d_{i,f}\geq1$ and $\Omega^d_{f,j}\geq1$. We then know that, $\forall f\in F$, $(X_i,X_f)\in E_d$ and $(X_f,X_j)\in E_d$. Then, we know that $\exists(X^{uv})_{u,v=1}^{u=U,\ v=1}=(X^{u1})_{u=1}^{U}$ (for $U=|F|$) such that $\forall u$, $X^{u1}\in\mathcal{X}$ and $(X_i,X^{u1})\in E_d$ and $(X^{u1},X_j)\in E_d$. We then have:
\begin{flalign*}
\lambda(\Omega_d,\Omega_d)_{i,j} &= max\big\{min\{\Omega^d_{i,f},\Omega^d_{f,j}\}\ |\ f\in F\big\} &\\
& = max\big\{min\{W_d\big((X_i, X_f)\big), W_d\big((X_f,X_j)\big)\}\ |\ f\in F\big\} &\\
& = max\big\{min\{W_d\big((X_i, X^{u1})\big), W_d\big((X^{u1},X_j)\big)\}\ |\ u\in\{1,\ldots,U\}\big\} &\\
& \textrm{ by construction of } F \textrm{ and }(X^{u1})_{u=1}^{U}
\end{flalign*}
Which is precisely our value in the second case of the formalized version of our claim. Combining those, our base case is complete.

\textbf{Inductive Case:} Let $t=n>2$, we then have: 
\[
\lambda_n(\underbrace{\Omega_d,\ldots,\Omega_d}_\text{n times})=\underbrace{\lambda(\cdots\lambda(\lambda(\lambda(}_\text{n-1 times}
    \Omega_d,\Omega_d),\Omega_d),\Omega_d,\ldots, \Omega_d)
\]

For the ease of notation, we will denote the matrix $\lambda_n(\Omega_d,\ldots,\Omega_d)=\lambda^n(\Omega_d)$. We then assume the following:
\[
\lambda^n(\Omega_d)_{i,j}=
\begin{cases}
0 & \textrm{if } \nexists (X^v)_{v=1}^{n-1} \textrm{ such that } \forall v,\ X^v\in\mathcal{X}\textrm{ and } \\
  & (X^v,X^{v+1})\in E_d, (X_i,X^1)\in E_d \textrm{ and } \\
  & (X^{n-1},X_j)\in E_d\\ 
\rho_{i,j} & \textrm{if }\exists (X^{uv})_{u,v=1}^{u=U,\ v=n-1}\ (\textrm{for some }U\geq1) \\
  & \textrm{such that }\forall u,\ \forall v,\ X^{uv}\in\mathcal{X} \textrm{ and } \\
  & (X^{uv},X^{u(v+1)})\in E_d, (X_i, X^{u1})\in E_d \textrm{ and } \\
  & (X^{u(n-1)},X_j)\in E_d, \textrm{ then: } \\
  & \rho_{i,j}=max\Big\{min\big\{W_d\big((X_i,X^{u1})\big),W_d\big((X^{uv},X^{u(v+1)})\big), \\
  & W_d\big((X^{u(n-1)},X_j)\big)\ |\ v\in\{1,\ldots,n-2\}\big\}\ |\ u\in\{1,\ldots,U\}\Big\}
\end{cases}
\]
Now let $t=n+1$, we then have:
\begin{flalign*}
\lambda_{n+1}(\underbrace{\Omega_d,\ldots,\Omega_d}_\text{n+1 times}) &=\underbrace{\lambda\bigl(\cdots\lambda(\lambda(\lambda(}_\text{n times}
    \Omega_d,\Omega_d),\Omega_d),\Omega_d,\ldots, \Omega_d \bigr) &\\
& =\lambda\bigl(\underbrace{\lambda(\cdots\lambda(\lambda(\lambda(}_\text{n-1 times}
    \Omega_d,\Omega_d),\Omega_d),\Omega_d,\ldots, \Omega_d),\Omega_d\bigr) &\\
& =\lambda\bigl(\lambda_n(\underbrace{\Omega_d,\ldots,\Omega_d}_\text{n times}),\Omega_d\bigr) &\\
& =\lambda\bigl(\lambda^n(\Omega_d),\Omega_d\bigr)
\end{flalign*}

Similarly, we denote $\lambda_{n+1}(\Omega_d,\ldots,\Omega_d)=\lambda^{n+1}(\Omega_d)$. We then have:
\begin{flalign*}
\lambda^{n+1}(\Omega_d)_{i,j} &=\lambda\bigl(\lambda^n(\Omega_d),\Omega_d\bigr)_{i,j} &\\
& =max\bigl\{min\{\lambda^n(\Omega_d)_{i,l},\Omega^d_{l,j}\}\ |\ l\in\{1,\ldots,k\}\bigr\} 
\end{flalign*}

We then have the following 2 cases:

\circled{1} If $\forall l\in\{1,\ldots,k\}$, $\lambda^n(\Omega_d)_{i,l}=0$ or $\Omega^d_{l,j}=0$, then $\lambda^{n+1}(\Omega_d)_{i,j}=0$. This is because $\forall i,j\in\{1,\ldots,k\}$, $\lambda^n(\Omega_d)_{i,j},\ \Omega^d_{i,j}\in\{0\}\cup\mathbb{R}_{\geq1}$ (since elements of $\lambda^n(\Omega_d)$ are simply drawn from elements of $\Omega_d$). Thus, if $\forall l\in\{1,\ldots,k\}$, $\lambda^n(\Omega_d)_{i,l}=0$ or $\Omega^d_{l,j}=0$, then $\lambda^{n+1}(\Omega_d)_{i,j}=max\{0,\ldots,0\}=0$.

\circled{2} If $\exists L\in\{1,\ldots,k\}$ (where $L\not=\varnothing$) such that $\forall l\in L$, $\lambda^n(\Omega_d)_{i,l}\geq1$ and $\Omega^d_{l,j}\geq1$, then: $\lambda^{n+1}(\Omega_d)_{i,j}=max\big\{min\{\lambda^n(\Omega_d)_{i,l}, \Omega^d_{l,j},0\}\ |\ l\in L\big\}=max\big\{min\{\lambda^n(\Omega_d)_{i,l}, \Omega^d_{l,j}\}\ |\ l\in L\big\}$

Again, considering \circled{1} and \circled{2}, we have precisely our formalized claim:

For \circled{1}, if $\forall l\in\{1,\ldots,k\}$, $\lambda^n(\Omega_d)_{i,l}=0$ or $\Omega^d_{l,j}=0$, then we know that, $\forall l$, $\Big[\nexists (X^v)_{v=1}^{n-1}$ such that $\forall v$, $X^v\in\mathcal{X}$ and $(X^v, X^{v+1})\in E_d$, $(X_i,X^1)\in E_d$ and $(X^{n-1},X_l)\in E_d\Big]$ or $\Big[ (X_l,X_j)\not\in E_d\Big]$. Hence, we know $\nexists (X^v)_{v=1}^{n}$ such that $\forall v$, $X^v\in \mathcal{X}$ and $(X^v, X^{v+1})\in E_d$, $(X_i, X^1)\in E_d$ and $(X^n,X_j)\in E_d$, which is precisely the condition in the first case of our formalized claim, then we have $\lambda^{n+1}(\Omega_d)=0$.

For \circled{2}, we have that $\lambda^{n+1}(\Omega_d)_{i,j}=max\big\{min\{\lambda^n(\Omega_d)_{i,l},\Omega^d_{l,j}\}\ |\ l\in L\big\}$, where $\varnothing\not=L\subseteq\{1,\ldots,k\}$ such that $\forall l\in L$, $\lambda^n(\Omega_d)_{i,l}\geq1$ and $\Omega^d_{l,j}\geq1$. By the definition of $\lambda_n$ function, we know that if $\lambda^n(\Omega_d)_{i,l}\not=\rho_{i,l}$, then $\lambda^n(\Omega_d)_{i,l}=0$. Taking the contrapositive, we have that if $\lambda^n(\Omega_d)_{i,l}\not=0$, then $\lambda^n(\Omega_d)_{i,l}=\rho_{i,l}$ (as $\rho_{i,l}$ is defined in the function). Since we have $0\not=\lambda^n(\Omega_d)_{i,l}\geq1$, we then know that $\lambda^n(\Omega_d)_{i,l}=\rho_{i,l}$. We then have that:
\begin{flalign*}
\lambda^{n+1}(\Omega_d)_{i,j} & =max\big\{min\{\lambda^n(\Omega_d)_{i,l},\Omega^d_{l,j}\}\ |\ l\in L\big\} &\\
& = max\big\{min\{\rho_{i,l},\Omega^d_{l,j}\}\ |\ l\in L\big\}
\end{flalign*}
We then let $l_m\in L$ denote the $m^{th}$ element of $L$ such that $L=\{l_1,\ldots,l_{|L|}\}$. Then, $\forall l_m\in L$, by definition, $\exists (X_{l_m}^{uv})_{u,v=1}^{v=n-1,\ u=U_m}$ such that $\forall u,\ \forall v$, $X_{l_m}^{uv}\in\mathcal{X}$ and $(X_{l_m}^{uv},X_{l_i}^{u(v+1)})\in E_d$, and $(X_i,X_{l_m}^{u1})\in E_d$ and $(X_{l_m}^{u(n-1)},X_{l_m})\in E_d$. Then applying the definitions of $\rho_{i,l}$ and $\Omega_{i,j}^d$, we have:

\[
\lambda^{n+1}(\Omega_d)_{i,j} =max
\left\{
  \begin{aligned}
    &min
      \left\{
      \begin{aligned}
        &max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X_{l_1}^{u1})\big), W_d\big((X_{l_1}^{uv},X_{l_1}^{u(v+1)})\big), \\
            &W_d\big((X_{l_1}^{u(n-1)},X_{l_1})\big)\ |\ v\in\{1,\ldots,n-2\}
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_1\}
          \end{aligned}
          \right\} \\
          &, W_d\big((X_{l_1},X_j)\big)
      \end{aligned}
      \right\} \\
      &\vdots \\
      &\vdots \\
    &min
      \left\{
      \begin{aligned}
        &max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X_{l_{|L|}}^{u1})\big), W_d\big((X_{l_{|L|}}^{uv},X_{l_{|L|}}^{u(v+1)})\big), \\
            &W_d\big((X_{l_{|L|}}^{u(n-1)},X_{l_{|L|}})\big)\ |\ v\in\{1,\ldots,n-2\}
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_{|L|}\}
          \end{aligned}
          \right\} \\
          &, W_d\big((X_{l_{|L|}},X_j)\big)
      \end{aligned}
      \right\} \\
  \end{aligned}
\right\}
\]

We then construct a sequence $(X^m)_{m=1}^M$ that's in bijection of the set $\{X_l\ |\ l\in L\}=\big\{X_{l_m}\ |\ m\in\{1,\ldots,|L|\}\big\}$. We then would have $M=|L|$, by construction. Using this notation, we have:
\[
\lambda^{n+1}(\Omega_d)_{i,j} =max
\left\{
  \begin{aligned}
    &min
      \left\{
      \begin{aligned}
        &max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{1(u)1})\big), W_d\big((X^{1uv},X^{1u(v+1)})\big), \\
            &W_d\big((X^{1u(n-1)},X^1)\big)\ |\ v\in\{1,\ldots,n-2\}
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_1\}
          \end{aligned}
          \right\} \\
          &, W_d\big((X^1,X_j)\big)
      \end{aligned}
      \right\} \\
      &\vdots \\
      &\vdots \\
    &min
      \left\{
      \begin{aligned}
        &max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{Mu1})\big), W_d\big((X^{Muv},X^{Mu(v+1)})\big), \\
            &W_d\big((X^{Mu(n-1)},X^M)\big)\ |\ v\in\{1,\ldots,n-2\}
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_M\}
          \end{aligned}
          \right\} \\
          &, W_d\big((X^M,X_j)\big)
      \end{aligned}
      \right\} \\
  \end{aligned}
\right\}
\]

Note that the superscripts are indexed as $X^{muv}$. Now, we fix a particular $m$ and apply \textit{Lemma C}, as follows:

\[
\begin{aligned}
&min
      \left\{
      \begin{aligned}
        &max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{mu1})\big), W_d\big((X^{muv},X^{mu(v+1)})\big), \\
            &W_d\big((X^{mu(n-1)},X^m)\big)\ |\ v\in\{1,\ldots,n-2\}
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_m\}
          \end{aligned}
          \right\} \\
          &, W_d\big((X^m,X_j)\big)
      \end{aligned}
      \right\} \\
      &= \\
&max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{mu1})\big), W_d\big((X^{muv},X^{mu(v+1)})\big), \\
            &W_d\big((X^{mu(n-1)},X^m)\big), W_d\big((X^m,X_j)\big)\ \\
            &|\ v\in\{1,\ldots,n-2\}
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_m\}
          \end{aligned}
          \right\}
\end{aligned}
\]

Fixing a particular $u\in\{1,\ldots,U_m\}$ (with the already fixed $m$), we construct a sequence $(X^{mug})_{g=1}^{g=n}$ such that $(X^{mug})_{g=1}^n=((X^{muv})_{v=1}^{n-1}),X^m)$. Hence, we have that $X^{mun}=X^m$ We then can write, for a fixed $m$, the following:

\[
\begin{aligned}
&max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{mu1})\big), W_d\big((X^{mug},X^{mu(g+1)})\big), \\
            &W_d\big((X^{mun},X_j)\big)\ |\ g\in\{1,\ldots,n-1\}\\
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_m\}
          \end{aligned}
          \right\}
\end{aligned}
\]



Using this equality, we then have:
\[
\lambda^{n+1}(\Omega_d)_{i,j} =max
\left\{
  \begin{aligned}
  &max
          \left\{
          \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{mu1})\big), W_d\big((X^{mug},X^{mu(g+1)})\big), \\
            &W_d\big((X^{mun},X_j)\big)\ |\ g\in\{1,\ldots,n-1\}\\
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_m\}
          \end{aligned}
          \right\} \\
  & m\in\{i,\ldots,M\}
  \end{aligned}
\right\}
\]

Grouping the two maximums, we have:
\[
\lambda^{n+1}(\Omega_d)_{i,j} =max
\left\{
  \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{mu1})\big), W_d\big((X^{mug},X^{mu(g+1)})\big), \\
            &W_d\big((X^{mun},X_j)\big)\ |\ g\in\{1,\ldots,n-1\}\\
            \end{aligned}
            \right\} \\
            &|\ u\in\{1,\ldots,U_m\},\ m\in\{i,\ldots,M\}
  \end{aligned}
\right\}
\]

Now, consider the sequence $(X^{mug})_{m,u,g=1}^{m=M,\ u=U_m,\ g=n}$. Fixing a particular $g$, we have $(X^{mu})_{m,u=1}^{m=M,\ u=U_m}$. Then construct a sequence $(X^z)_{z=1}^Z$ in bijection of $(X^{mu})_{m,u=1}^{m=M,\ u=U_m}$ such that $Z=\sum_{m\in M}U_m$. We then have the following: 

\[
\lambda^{n+1}(\Omega_d)_{i,j} =max
\left\{
  \begin{aligned}
            &min\left\{
            \begin{aligned}
            &W_d\big((X_i,X^{z1})\big), W_d\big((X^{zg},X^{z(g+1)})\big), \\
            &W_d\big((X^{zn},X_j)\big)\ |\ g\in\{1,\ldots,n-1\}\\
            \end{aligned}
            \right\} \\
            &|\ z\in\{1,\ldots,Z\}
  \end{aligned}
\right\}
\]

With \circled{1} and \circled{2}, we have the following:

\[
\lambda^{n+1}(\Omega_d)_{i,j}=
\begin{cases}
0 & \textrm{if } \nexists (X^v)_{v=1}^{n} \textrm{ such that } \forall v,\ X^v\in\mathcal{X}\textrm{ and } \\
  & (X^v,X^{v+1})\in E_d, (X_i,X^1)\in E_d \textrm{ and } \\
  & (X^{n},X_j)\in E_d\\ 
\rho_{i,j} & \textrm{if }\exists (X^{zg})_{z,g=1}^{z=Z,\ g=n}\ (\textrm{for some }Z\geq1) \\
  & \textrm{such that }\forall z,\ \forall g,\ X^{zg}\in\mathcal{X} \textrm{ and } \\
  & (X^{zg},X^{z(g+1)})\in E_d, (X_i, X^{z1})\in E_d \textrm{ and } \\
  & (X^{zn},X_j)\in E_d, \textrm{ then: } \\
  & \rho_{i,j}=max\Big\{min\big\{W_d\big((X_i,X^{z1})\big),W_d\big((X^{zg},X^{z(g+1)})\big), \\
  & W_d\big((X^{zn},X_j)\big)\ |\ g\in\{1,\ldots,n-1\}\big\}\ |\ z\in\{1,\ldots,Z\}\Big\}
\end{cases}
\]
Hence, our inductive case is proven.
\end{proof}

We then define a $k\times k$ matrix $\Lambda$ such that $\forall i,j\in\{1,\ldots,k\}$, we have that $\Lambda_{i,j}=max\{\Omega_{d_{i,j}},\lambda^2(\Omega_d)_{i,j},\lambda^3(\Omega_d)_{i,j},\ldots,\lambda^{k-1}(\Omega_d)_{i,j}\}$ (i.e. $\Lambda$ is the element-wise maximization of matrices $\Omega_{d_{i,j}},\lambda^2(\Omega_d),\ldots,\lambda^{k-1}(\Omega_d)$). 

\textit{Corollary 3.1:} $\Lambda_{i,j}=\varrho_{i,j}\geq1$, $\forall i,j\{1,\ldots,k\}$ such that $i\not=j$, represents the weight of the strongest edge among the weakest edges of all walks, no matter the number of edges in the walks, from $X_i$ to $X_j$ on $G_d$.

This comes directly from \textit{Theorem 3}, as each $\lambda^t(\Omega_d)_{i,j}$ represents the weight of the strongest edge among the weakest edge of \textit{walks of} $t$ \textit{edges} from $X_i$ to $X_j$ on $G_d$. Taking the maximum for $t\in\{2,\ldots,k-1\}$ would cover the strongest edge among weakest edge of all walks with two edges or greater (regardless of number of edges) and $\Omega_{d_{i,j}}$ represent exactly the case with walks of only one edge. We iterate up to $k-1$ as there are only $k$ total of vertices and we have $i\not=j$.

\textit{Corollary 3.2:} If $\Lambda_{i,j}=0$, then $\nexists$ any path from $X_i$ to $X_j$ on $G_d$.

This also comes directly from \textit{Theorem 3}. Since we have $\Lambda_{i,j}=0$, then $max\{\Omega_{d_{i,j}},\lambda^2(\Omega_d)_{i,j},\lambda^3(\Omega_d)_{i,j},\ldots,\lambda^{k-1}(\Omega_d)_{i,j}\}=0$ and hence $\Omega_{d_{i,j}}=\lambda^2(\Omega_d)_{i,j}=\lambda^3(\Omega_d)_{i,j}=\ldots=\lambda^{k-1}(\Omega_d)_{i,j}=0$.

\textit{Theorem 4:} $\forall i\in\{1,\ldots,k\}$, $\lambda(\Lambda,\Omega_s)_{i,i}=0$ if and only if the dataset $(\mathcal{X},\mathcal{P})$ has no GARP violations. 

\textit{Theorem 5:} If $\exists i\in\{1,\ldots,k\}$ such that $\lambda(\Lambda,\Omega_s)_{i,i}\geq1$, then $\lambda(\Lambda,\Omega_s)_{i,i}$ represents the weakest edge of the worst (strongest) GARP violation involving $X_i$.

\textit{Theorem 6:} Let $\psi = max\big\{\lambda(\Lambda,\Omega_s)_{i,i}\ |\ i\in\{1,\ldots,k\}\big\}$, then let $c=\frac{1}{\psi}$. The $c$ is the CCEI score related to the dataset $(\mathcal{X},\mathcal{P})$.
\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}